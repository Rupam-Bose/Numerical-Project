# ğŸ“˜ Table of Contents

<details>
<summary><b><a href = "#Solution-of-Linear-Equations">Solution of Linear Equations</a></b></summary>
<br>

<details>
<summary><a href = "#Gauss-Elimination-Method">ğŸ”¹ Gauss Elimination Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Gauss-Elimination-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Gauss-Elimination-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Gauss-Elimination-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Gauss-Elimination-Method-Output)  

</details>
<br>

<details>
<summary><a href = "#Gauss-Jordan-Elimination-Method">ğŸ”¹ Gauss Jordan Elimination Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Gauss-Jordan-Elimination-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Gauss-Jordan-Elimination-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Gauss-Jordan-Elimination-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Gauss-Jordan-Elimination-Method-Output)  

</details>
<br>

<details>
<summary><a href = "#LU-Factorization-Method">ğŸ”¹ LU Factorization Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-LU-Factorization-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-LU-Factorization-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-LU-Factorization-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-LU-Factorization-Method-Output)  

</details>
<br>

<details>
<summary><a href = "#Matrix-Inversion-Method">ğŸ”¹ Matrix Inversion Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Matrix-Inversion-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Matrix-Inversion-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Matrix-Inversion-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ğŸ“¤ [Output](#-Matrix-Inversion-Method-Output)  

</details>

</details>


<details>
<summary><b><a href = "#Solution-of-Non-Linear-Equations">Solution of Non-Linear Equations</a></b></summary>
<br>

<details>
<summary><a href = "#Bisection-Method">ğŸ”¹ Bisection Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Bisection-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Bisection-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Bisection-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Bisection-Method-Output)  

</details>
<br>

<details>
<summary><a href = "#False-Position-Method">ğŸ”¹ False Position Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-False-Position-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-False-Position-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-False-Position-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-False-Position-Method-Output)  

</details>
<br>


<details>
<summary><a href = "#Newton-Raphson-Method">ğŸ”¹ Newton-Raphson Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Newton-Raphson-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Newton-Raphson-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Newton-Raphson-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Newton-Raphson-Method-Output)  

</details>
<br>

<details>
<summary><a href = "#Secant-Method">ğŸ”¹ Secant Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Secant-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Secant-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Secant-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Secant-Method-Output)  

</details>
<br>

</details>

<details>
<summary><b><a href = "#Curve-Fitting">Curve Fitting: Regression</a></b></summary>
<br>

<details>
<summary><a href = "#Least-Square-Regression-Linear-Equation">ğŸ”¹ Least Square Regression (Linear Equation)</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Least-Square-Regression-Linear-Equation-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Least-Square-Regression-Linear-Equation-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Least-Square-Regression-Linear-Equation-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Least-Square-Regression-Linear-Equation-Output)  

</details>
<br>

<details>
<summary><a href = "#Least-Square-Regression-Transcendental-Equation">ğŸ”¹ Least Square Regression (Transcendental Equation)</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Least-Square-Regression-Transcendental-Equation-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Least-Square-Regression-Transcendental-Equation-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Least-Square-Regression-Transcendental-Equation-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Least-Square-Regression-Transcendental-Equation-Output)  

</details>
<br>


<details>
<summary><a href = "#Least-Square-Regression-Polynomial-Equation">ğŸ”¹ Least Square Regression (Polynomial Equation)</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Least-Square-Regression-Polynomial-Equation-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Least-Square-Regression-Polynomial-Equation-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Least-Square-Regression-Polynomial-Equation-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Least-Square-Regression-Polynomial-Equation-Output)  

</details>
<br>

</details>

<details>
<summary><b><a href = "#Interpolation">Interpolation</a></b></summary>
<br>

<details>
<summary><a href = "#Newton-Forward-Interpolation-Method">ğŸ”¹ Newton Forward Interpolation Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Newton-Forward-Interpolation-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Newton-Forward-Interpolation-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Newton-Forward-Interpolation-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Newton-Forward-Interpolation-Method-Output)  

</details>
<br>

<details>
<summary><a href = "#Newton-Backward-Interpolation-Method">ğŸ”¹ Newton Backward Interpolation Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Newton-Backward-Interpolation-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Newton-Backward-Interpolation-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Newton-Backward-Interpolation-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Newton-Backward-Interpolation-Method-Output)  

</details>
<br>


<details>
<summary><a href = "#Newton-Divided-Difference-Interpolation-Method">ğŸ”¹ Newton Divided Difference Interpolation Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Newton-Divided-Difference-Interpolation-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Newton-Divided-Difference-Interpolation-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Newton-Divided-Difference-Interpolation-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Newton-Divided-Difference-Interpolation-Method-Output)  

</details>
<br>

</details>

<details>
<summary><b><a href = "#Solution-of-Ordinary-Differential-Equation">Solution of Ordinary Differential Equation</a></b></summary>
<br>

<details>
<summary><a href = "#Runge-Kutta-Method">ğŸ”¹ Runge Kutta Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Runge-Kutta-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Runge-Kutta-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Runge-Kutta-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Runge-Kutta-Method-Output)  

</details>
<br>

</details>

<details>
<summary><b><a href = "#Integration">Integration</a></b></summary>
<br>

<details>
<summary><a href = "#Simpson-13-Rule">ğŸ”¹ Simpson 1/3 Rule</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Simpson-13-Rule-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Simpson-13-Rule-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Simpson-13-Rule-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Simpson-13-Rule-Output)  

</details>
<br>
<details>
<summary><a href = "#Simpson-38-Rule">ğŸ”¹ Simpson 3/8 Rule</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Simpson-38-Rule-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Simpson-38-Rule-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Simpson-38-Rule-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Simpson-38-Rule-Output)  

</details>
<br>

</details>

<details>
<summary><b><a href = "#Differentiation">Differentiation</a></b></summary>
<br>

<details>
<summary><a href = "#Differentiation-Using-Forward-Interpolation">ğŸ”¹ Differentiation Using Forward Interpolation</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Differentiation-Using-Forward-Interpolation-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Differentiation-Using-Forward-Interpolationn-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Differentiation-Using-Forward-Interpolation-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Differentiation-Using-Forward-Interpolation-Output)  

</details>
<br>

<details>
<summary><a href = "#Differentiation-Using-Backward-Interpolation">ğŸ”¹ Differentiation Using Backward Interpolation</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Differentiation-Using-Backward-Interpolation-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Differentiation-Using-Backward-Interpolationn-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Differentiation-Using-Backward-Interpolation-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Differentiation-Using-Backward-Interpolation-Output)  

</details>
<br>
</details>

---

<br>

# Project Structure
```
Numerical-Methods-Project/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ 01_Solution_of_Linear_Equations/
â”‚   â”‚
â”‚   â”œâ”€â”€ Gauss_Elimination/
â”‚   â”‚   â”œâ”€â”€ theory.md
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”‚   â””â”€â”€ gauss_elimination.cpp
â”‚   â”‚   â”œâ”€â”€ input.txt
â”‚   â”‚   â””â”€â”€ output.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ Gauss_Jordan_Elimination/
â”‚   â”‚   â”œâ”€â”€ theory.md
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”‚   â””â”€â”€ gauss_jordan.cpp
â”‚   â”‚   â”œâ”€â”€ input.txt
â”‚   â”‚   â””â”€â”€ output.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ LU_Factorization/
â”‚   â”‚   â”œâ”€â”€ theory.md
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”‚   â””â”€â”€ lu_factorization.cpp
â”‚   â”‚   â”œâ”€â”€ input.txt
â”‚   â”‚   â””â”€â”€ output.txt
â”‚   â”‚
â”‚   â””â”€â”€ Matrix_Inversion/
â”‚       â”œâ”€â”€ theory.md
â”‚       â”œâ”€â”€ code/
â”‚       â”‚   â””â”€â”€ matrix_inversion.cpp
â”‚       â”œâ”€â”€ input.txt
â”‚       â””â”€â”€ output.txt
â”‚
â”œâ”€â”€ 02_Solution_of_Non_Linear_Equations/
â”‚   â”‚
â”‚   â”œâ”€â”€ Bisection_Method/
â”‚   â”‚   â”œâ”€â”€ theory.md
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”‚   â””â”€â”€ bisection.cpp
â”‚   â”‚   â”œâ”€â”€ input.txt
â”‚   â”‚   â””â”€â”€ output.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ False_Position_Method/
â”‚   â”‚   â”œâ”€â”€ theory.md
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”‚   â””â”€â”€ false_position.cpp
â”‚   â”‚   â”œâ”€â”€ input.txt
â”‚   â”‚   â””â”€â”€ output.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ Newton_Raphson_Method/
â”‚   â”‚   â”œâ”€â”€ theory.md
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”‚   â””â”€â”€ newton_raphson.cpp
â”‚   â”‚   â”œâ”€â”€ input.txt
â”‚   â”‚   â””â”€â”€ output.txt
â”‚   â”‚
â”‚   â””â”€â”€ Secant_Method/
â”‚       â”œâ”€â”€ theory.md
â”‚       â”œâ”€â”€ code/
â”‚       â”‚   â””â”€â”€ secant.cpp
â”‚       â”œâ”€â”€ input.txt
â”‚       â””â”€â”€ output.txt
â”‚
â”œâ”€â”€ 03_Curve_Fitting_Regression/
â”‚   â”‚
â”‚   â”œâ”€â”€ Least_Square_Linear/
â”‚   â”‚   â”œâ”€â”€ theory.md
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”‚   â””â”€â”€ ls_linear.cpp
â”‚   â”‚   â”œâ”€â”€ input.txt
â”‚   â”‚   â””â”€â”€ output.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ Least_Square_Transcendental/
â”‚   â”‚   â”œâ”€â”€ theory.md
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”‚   â””â”€â”€ ls_transcendental.cpp
â”‚   â”‚   â”œâ”€â”€ input.txt
â”‚   â”‚   â””â”€â”€ output.txt
â”‚   â”‚
â”‚   â””â”€â”€ Least_Square_Polynomial/
â”‚       â”œâ”€â”€ theory.md
â”‚       â”œâ”€â”€ code/
â”‚       â”‚   â””â”€â”€ ls_polynomial.cpp
â”‚       â”œâ”€â”€ input.txt
â”‚       â””â”€â”€ output.txt
â”‚
â”œâ”€â”€ 04_Interpolation/
â”‚   â”‚
â”‚   â”œâ”€â”€ Newton_Forward/
â”‚   â”‚   â”œâ”€â”€ theory.md
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”‚   â””â”€â”€ newton_forward.cpp
â”‚   â”‚   â”œâ”€â”€ input.txt
â”‚   â”‚   â””â”€â”€ output.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ Newton_Backward/
â”‚   â”‚   â”œâ”€â”€ theory.md
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”‚   â””â”€â”€ newton_backward.cpp
â”‚   â”‚   â”œâ”€â”€ input.txt
â”‚   â”‚   â””â”€â”€ output.txt
â”‚   â”‚
â”‚   â””â”€â”€ Newton_Divided_Difference/
â”‚       â”œâ”€â”€ theory.md
â”‚       â”œâ”€â”€ code/
â”‚       â”‚   â””â”€â”€ newton_divided_difference.cpp
â”‚       â”œâ”€â”€ input.txt
â”‚       â””â”€â”€ output.txt
â”‚
â”œâ”€â”€ 05_Ordinary_Differential_Equations/
â”‚   â”‚
â”‚   â””â”€â”€ Runge_Kutta_Method/
â”‚       â”œâ”€â”€ theory.md
â”‚       â”œâ”€â”€ code/
â”‚       â”‚   â””â”€â”€ runge_kutta.cpp
â”‚       â”œâ”€â”€ input.txt
â”‚       â””â”€â”€ output.txt
â”‚
â”œâ”€â”€ 06_Integration/
â”‚   â”‚
â”‚   â”œâ”€â”€ Simpson_One_Third_Rule/
â”‚   â”‚   â”œâ”€â”€ theory.md
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”‚   â””â”€â”€ simpson_1_3.cpp
â”‚   â”‚   â”œâ”€â”€ input.txt
â”‚   â”‚   â””â”€â”€ output.txt
â”‚   â”‚
â”‚   â””â”€â”€ Simpson_Three_Eighth_Rule/
â”‚       â”œâ”€â”€ theory.md
â”‚       â”œâ”€â”€ code/
â”‚       â”‚   â””â”€â”€ simpson_3_8.cpp
â”‚       â”œâ”€â”€ input.txt
â”‚       â””â”€â”€ output.txt
â”‚
â”œâ”€â”€ 07_Differentiation/
    â”‚
    â”œâ”€â”€ Forward_Interpolation_Method/
    â”‚   â”œâ”€â”€ theory.md
    â”‚   â”œâ”€â”€ code/
    â”‚   â”‚   â””â”€â”€ differentiation_forward.cpp
    â”‚   â”œâ”€â”€ input.txt
    â”‚   â””â”€â”€ output.txt
    â”‚
    â””â”€â”€ Backward_Interpolation_Method/
        â”œâ”€â”€ theory.md
        â”œâ”€â”€ code/
        â”‚   â””â”€â”€ differentiation_backward.cpp
        â”œâ”€â”€ input.txt
        â””â”€â”€ output.txt


```
<br>

[Back to Top](#-Table-of-Contents)

---

# Solution of Linear Equations

A linear Equations with n variable has the form:
```
aâ‚xâ‚ + aâ‚‚xâ‚‚ + â€¦ + aâ‚™xâ‚™ = b
```
To find a unique solution, we need a set of n such independent equations. This set of equations is known as a system of equations. A system of n linear equations is written as:
```
aâ‚â‚xâ‚ + aâ‚â‚‚xâ‚‚ + ... + aâ‚â‚™xâ‚™ = bâ‚
aâ‚‚â‚xâ‚ + aâ‚‚â‚‚xâ‚‚ + ... + aâ‚‚â‚™xâ‚™ = bâ‚‚
.
.
.
aâ‚™â‚xâ‚ + aâ‚™â‚‚xâ‚‚ + ... + aâ‚™â‚™xâ‚™ = bâ‚™
```
The matrix form of a linear system of n linear equations can written as:
```
Ax = b
```
where, 

`A = [aáµ¢â±¼]` - A is the coefficient matrix of order `n x n`

`x = [xâ‚ xâ‚‚ ... xâ‚™]áµ€` - x is the vector of unknowns

`b = [bâ‚ bâ‚‚ ... bâ‚™]áµ€` - b is the constant matrix


The techniques for solving system of linear equations are:
  - Elimination approach
  - Iterative approach

The most commonly used elimination methods are:
  - Gauss elimination method
  - Gauss-Jordan elimination method
  - LU decomposition method
  - Matrix inversion method
    
<br>
---

## Gauss Elimination Method

<br>

### ğŸ“– Gauss Elimination Method Theory

Gauss elimination method is a row reduction algorithm to solve linear systems. It is operated on augmented matrix,which combine the coefficient matrix and constant matrix. 

The procedure comprises of two phases:

  1. *Forward Elimination Phase*: In this phase, the augmented matrix `[A | b]` is converted to upper triangular matrix through a series of elemintary row operations. 

  2. *Backward Substitution Phase*: After obtaining the reduced form of the matrix, the solution vector is found by back substitution process, starting from the last equation and moving upward.
     
<br>

### ğŸ¤– Algorithm

  1. Perform partial pivoting for each row to select the largest element as the pivot of that row by swapping operation. The pivoting is done to avoid division by zero and round-off errors.

  2. Convert the augemented matrix `[A | b]` into `[U | c]` where U is the upper triangular matrix. For each pivot row, eliminate all the elements below the pivot using the elementary row operation:
  ```
    A[j] = A[j] - (A[j][i] / A[i][i]) * A[i]
  ``` 

  3. Use back substitution to solve `Ux = c` to obtain `x`. We start from the last variable and substitute back into previous equations until all the unknowns are found.

  4. Before back substitution, check if the system has a unique solution by verifying `U[n-1][n-1]` and `c[n-1]`. 

     - If U[n-1][n-1] = 0 and c[n-1] = 0, the system has infinite solutions.

     - If U[n-1][n-1] = 0 and c[n-1] != 0, the system has no solution.

     - If U[n-1][n-1] != 0 and c[n-1] != 0, the system has unique solution.

<br>

### ğŸ’» Gauss Elimination Method Code

```cpp
#include <bits/stdc++.h>
#include <fstream>
using namespace std;

//-----Forward Elimination phase: Converting the Conefficient Matrix into Upper Triangular Matrix with pivoting------
void forwardd(vector<vector<double>> &cof)
{
    int n = cof.size();
    for (int i = 0; i < n - 1; i++)
    {
        int pivot = i;
        for (int j = i + 1; j < n; j++)
        {
            if (fabs(cof[j][i]) > fabs(cof[pivot][i]))
                pivot = j;
        }

        if (pivot != i)
            swap(cof[i], cof[pivot]);

        if (fabs(cof[i][i]) < 1e-9)
            continue;

        double x = cof[i][i];

        for (int k = i + 1; k < n; k++)
        {
            double y = cof[k][i];
            for (int j = i; j < n + 1; j++)
            {
                cof[k][j] = cof[k][j] - cof[i][j] * y / x;
            }
        }
    }
}

//------Back Substition Phase: Finding the solution vector------
void backwardd(vector<vector<double>> &cof, vector<double> &ans)
{
    int n = cof.size();
    for (int i = n - 1; i >= 0; i--)
    {
        double sum = 0;
        for (int j = i + 1; j < n; j++)
        {
            sum += ans[j] * cof[i][j];
        }
        ans[i] = (cof[i][n] - sum) / cof[i][i];
    }
}

//------Printing the Linear System-------
void systemPrint(vector<vector<double>> &cof, ofstream &fout)
{
    fout << "The Linear System: \n";
    int n = cof.size();
    for (int i = 0; i < n; i++)
    {
        fout << cof[i][0] << "*x" << 1 << " ";
        for (int j = 1; j < n; j++)
        {
            if (cof[i][j] >= 0)
                fout << " + ";
            fout << cof[i][j] << "*x" << j + 1 << " ";
        }
        fout << " = " << cof[i][n] << '\n';
    }
}

void solve(ifstream &fin, ofstream &fout)
{
    //------Reading the number of linear equations of the linear system------
    int n;
    fin >> n;

    vector<vector<double>> cof(n, vector<double>(n + 1));

    //------Reading the Conefficient Matrix------
    for (int i = 0; i < n; i++)
    {
        for (int j = 0; j < n + 1; j++)
        {
            fin >> cof[i][j];
        }
    }

    systemPrint(cof, fout);

    forwardd(cof);

    //------Handling the solutions type of the system------
    if (cof[n - 1][n - 1] == 0 && cof[n - 1][n])
    {
        fout << "\nThe system has NO SOLUTION.\n";
    }

    else if (cof[n - 1][n - 1] == 0 && cof[n - 1][n] == 0)
    {
        fout << "\nThe system has INFINITE SOLUTIONS.\n";
    }

    else
    {
        fout << "\nThe system has UNIQUE SOLUTION.\n";
        vector<double> ans(n, 0);
        backwardd(cof, ans);

        //------Printing the solutions of the System------
        fout << "\nSolutions: \n";
        for (int i = 0; i < n; i++)
        {
            fout << "x" << i + 1 << " = " << ans[i] << '\n';
        }
    }
}

int main()
{
    ifstream fin("input.txt");
    ofstream fout("output.txt");

    fout << "Solution of Linear System using Gauss Elimination Method\nMultiple Testcases\n\n";

    if (!fin)
    {
        fout << "File not found.\n";
        return 0;
    }

    //------Mutiple Test Cases------
    int t;
    fin >> t;

    int cse = 1;

    while (t--)
    {
        for (int i = 0; i < 45; i++)
            fout << "=";
        fout << '\n';
        fout << "Test case: " << cse << "\n";
        solve(fin, fout);
        cse++;
    }
}
```
<br>

### ğŸ“ Gauss Elimination Method Input
```
4
3
3 6 1 16
2 4 3 13
1 3 2 9
2
1 1 2
2 2 4
2
1 1 2
2 2 5
5
2 1 -1 3 2 9
1 3 2 -1 1 8
3 2 4 1 -2 20
2 1 3 2 1 17
1 -1 2 3 4 15
```
<br>

### ğŸ“¤ Gauss Elimination Method Output
```
Solution of Linear System using Gauss Elimination Method
Multiple Testcases

=============================================
Test case: 1
The Linear System: 
3*x1  + 6*x2  + 1*x3  = 16
2*x1  + 4*x2  + 3*x3  = 13
1*x1  + 3*x2  + 2*x3  = 9

The system has UNIQUE SOLUTION.

Solutions: 
x1 = 1
x2 = 2
x3 = 1
=============================================
Test case: 2
The Linear System: 
1*x1  + 1*x2  = 2
2*x1  + 2*x2  = 4

The system has INFINITE SOLUTIONS.
=============================================
Test case: 3
The Linear System: 
1*x1  + 1*x2  = 2
2*x1  + 2*x2  = 5

The system has NO SOLUTION.
=============================================
Test case: 4
The Linear System: 
2*x1  + 1*x2 -1*x3  + 3*x4  + 2*x5  = 9
1*x1  + 3*x2  + 2*x3 -1*x4  + 1*x5  = 8
3*x1  + 2*x2  + 4*x3  + 1*x4 -2*x5  = 20
2*x1  + 1*x2  + 3*x3  + 2*x4  + 1*x5  = 17
1*x1 -1*x2  + 2*x3  + 3*x4  + 4*x5  = 15

The system has UNIQUE SOLUTION.

Solutions: 
x1 = 5.15385
x2 = -1
x3 = 2.26154
x4 = -0.138462
x5 = 1.18462
```
<br>

---

### ğŸ¯ Accuracy Consideration

- Floating point error may occur due to division and subtraction operations.

- Partial pivoting helps avoiding division by zero and round-off errors.

- Using a tolerant value(i.e `1e-9`) helps avoid division by small numbers.
<br>

### â• Advantages

- Is straightforward and easy to understand.

- Can be easily implemented in computer programs.

- Widely applicable to various types of systems of linear equations.
<br>

### â– Disadvantages

- Can be computationally expensive for large systems with many equations.

- The method may suffer from round off errors while dealing with floating point arithmetic.
<br>

### ğŸš€ Applications

- Solving linear systems in Physics and Engineering

- Determining rank and determinant of matrices

- Used as foundation for LU decomposition and matrix inversion
<br>

[Back to Top](#-Table-of-Contents)

---

## Gauss-Jordan Elimination Method

<br>

### ğŸ“– Gauss-Jordan Elimination Method Theory

Gauss-Jordan elimination method is an extension of Gauss elimination used to solve systems of linear equations. Both methods use systematic elimination of variables through row operations.

The key difference between the methods is that Gauss elimination eliminates variables only below the pivot elements, producing an upper triangular matrix, while Gauss-Jordan elimination eliminates variables from all other rows (both above and below the pivot), resulting in a reduced row echelon form (identity matrix). This allows us to obtain the unknowns directly without employing back substitution.

The method transforms the augmented matrix `[A | b]` into `[I | x]` where I is the identity matrix and x is the solution vector. 

<br>

### ğŸ¤– Algorithm

 1. Select the pivot element for each row by swapping the rows for the largest element of that column.

 2. For each pivot row, normalize the row by its pivot element to make the diagonal 1.

 3. Eliminate the variable from all the other rows above and below the pivot to convert the augmented matrix `[A|b]` into `[I|c]` where I is the identity matrix.

 4. After obtaining the diagonal matrix, the solution is directly read from the last column of the resulting augmented matrix.
 ```
 xâ‚ = câ‚, xâ‚‚ = câ‚‚, â€¦, xâ‚™ = câ‚™
 ```
<br>

### ğŸ’» Gauss-Jordan Elimination Method Code

```cpp
#include <bits/stdc++.h>
#include <fstream>
using namespace std;

//-----Forward Elimination phase: Converting the Conefficient Matrix into Identity Matrix with partial pivoting------
void forwardd(vector<vector<double>> &cof)
{
    int n = cof.size();
    for (int i = 0; i < n; i++)
    {
        int pivot = i;

        for (int j = i + 1; j < n; j++)
        {
            if (fabs(cof[j][i]) > fabs(cof[pivot][i]))
                pivot = j;
        }

        if (pivot != i)
            swap(cof[i], cof[pivot]);

        if (fabs(cof[i][i]) < 1e-9)
            continue;

        double x = cof[i][i];

        for (int j = i; j < n + 1; j++)
        {
            if (cof[i][j])
                cof[i][j] /= x;
        }

        for (int k = 0; k < n; k++)
        {
            if (k != i)
            {
                double y = cof[k][i];
                for (int j = 0; j < n + 1; j++)
                {
                    if (k != i)
                        cof[k][j] -= cof[i][j] * y;
                }
            }
        }
    }
}

//------Printing the Linear System-------
void systemPrint(vector<vector<double>> &cof, ofstream &fout)
{
    fout << "The Linear System: \n";
    int n = cof.size();
    for (int i = 0; i < n; i++)
    {
        fout << cof[i][0] << "*x" << 1 << " ";

        for (int j = 1; j < n; j++)
        {
            if (cof[i][j] >= 0)
                fout << " + ";
            fout << cof[i][j] << "*x" << j + 1 << " ";
        }

        fout << " = " << cof[i][n] << '\n';
    }
}

void solve(ifstream &fin, ofstream &fout)
{
    //------Reading the number of linear equations of the linear system------
    int n;
    fin >> n;

    vector<vector<double>> cof(n, vector<double>(n + 1));

    //------Reading the Conefficient Matrix------
    for (int i = 0; i < n; i++)
    {
        for (int j = 0; j < n + 1; j++)
        {
            fin >> cof[i][j];
        }
    }

    systemPrint(cof, fout);

    forwardd(cof);

    //------Handling the solutions type of the system------
    if (cof[n - 1][n - 1] == 0 && cof[n - 1][n])
    {
        fout << "\nThe system has NO SOLUTION.\n";
    }

    else if (cof[n - 1][n - 1] == 0 && cof[n - 1][n] == 0)
    {
        fout << "\nThe system has INFINITE SOLUTIONS.\n";
    }

    else
    {
        fout << "\nThe system has UNIQUE SOLUTION.\n";

        vector<double> ans;

        for (int i = 0; i < n; i++)
        {
            ans.push_back(cof[i][n]);
        }

        //------Printing the solutions of the System------
        fout << "\nSolutions: \n";
        for (int i = 0; i < n; i++)
        {
            fout << "x" << i + 1 << " = " << ans[i] << '\n';
        }
    }
}

int main()
{
    ifstream fin("input.txt");
    ofstream fout("output.txt");

    fout << "Solution of Linear System using Gauss-Jordan Elimination Method\nMultiple Testcases\n\n";

    if (!fin)
    {
        fout << "File not found.\n";
        return 0;
    }

    //------Mutiple Test Cases------
    int t;
    fin >> t;

    int cse = 1;

    while (t--)
    {
        for (int i = 0; i < 45; i++)
            fout << "=";
        fout << '\n';
        fout << "Test case: " << cse << "\n";
        solve(fin, fout);
        cse++;
    }
}
```
<br>

### ğŸ“ Gauss-Jordan Elimination Method Input
```
4
3
3 6 1 16
2 4 3 13
1 3 2 9
2
1 1 2
2 2 4
2
1 1 2
2 2 5
5
2 1 -1 3 2 9
1 3 2 -1 1 8
3 2 4 1 -2 20
2 1 3 2 1 17
1 -1 2 3 4 15
```
<br>

### ğŸ“¤ Gauss-Jordan Elimination Method Output
```
Solution of Linear System using Gauss-Jordan Elimination Method
Multiple Testcases

=============================================
Test case: 1
The Linear System: 
3*x1  + 6*x2  + 1*x3  = 16
2*x1  + 4*x2  + 3*x3  = 13
1*x1  + 3*x2  + 2*x3  = 9

The system has UNIQUE SOLUTION.

Solutions: 
x1 = 1
x2 = 2
x3 = 1
=============================================
Test case: 2
The Linear System: 
1*x1  + 1*x2  = 2
2*x1  + 2*x2  = 4

The system has INFINITE SOLUTIONS.
=============================================
Test case: 3
The Linear System: 
1*x1  + 1*x2  = 2
2*x1  + 2*x2  = 5

The system has NO SOLUTION.
=============================================
Test case: 4
The Linear System: 
2*x1  + 1*x2 -1*x3  + 3*x4  + 2*x5  = 9
1*x1  + 3*x2  + 2*x3 -1*x4  + 1*x5  = 8
3*x1  + 2*x2  + 4*x3  + 1*x4 -2*x5  = 20
2*x1  + 1*x2  + 3*x3  + 2*x4  + 1*x5  = 17
1*x1 -1*x2  + 2*x3  + 3*x4  + 4*x5  = 15

The system has UNIQUE SOLUTION.

Solutions: 
x1 = 5.15385
x2 = -1
x3 = 2.26154
x4 = -0.138462
x5 = 1.18462
```
<br>

### ğŸ¯ Accuracy Consideration

- Small pivots can cause round-off errors as the method involves division by pivot elements

- Partial pivoting helps maintain numerical stability by choosing the largest available element

- Direct solution avoids accumulation of back substitution errors
<br>

### â• Advantages

- Simple and straightforward to implement for small to medium-sized systems

- Produces the solution directly without requiring back substitution

- Can clearly identify whether a system has unique, no, or infinitely many solutions

- Forms the theoretical basis for methods like matrix inversion
<br>

### â– Disadvantages

- Computationally more expensive for large systems.

- Requires more arithmetic operations than standard Gauss elimination.

- Sensitive to round-off errors without proper pivoting.
<br>

### ğŸš€ Applications

- Solving systems of linear equations in engineering and physics

- Finding rank and determinant of matrices

- Circuit analysis and network flow problems
<br>

[Back to Top](#-Table-of-Contents)

---

## LU-Factorization Method

<br>

### ğŸ“– LU-Factorization Method Theory

LU decomposition or factorization is a powerful numerical method for solving systems of linear equations. This method factorizes the coefficient matrix `A` into the product of two triangular matrices: a lower triangular matrix `L` and an upper triangular matrix `U`, such that `A = LU`.

The procedure consists of three main phase:
  1. *Decomposition Phase*: Decomposing the square coefficient matrix into lower triangular matrix `L` and upper triangular matrix `U` such that `A = LU`. The `L` matrix has ones on its diagonal, and `U` is obtained using Gauss elimination with partial pivoting.

  2. *Forward Substitution*: Solving `Ly = b` for the intermediate vector y using the lower triangular matrix. This is straightforward since `L` is lower triangular.

  3. *Backward Substitution*: Solving `Ux = y` for the solution vector `x` using the upper triangular matrix. This gives the final solution to the original system.
     
<br>

### ğŸ¤– Algorithm

 1. Decomposition (Finding U and L matrices): 
    - Initialize: Copy coefficient matrix A into U. Initialize L as a zero matrix with diagonal elements set to 1.
    - Compute Upper Triangular Matrix U** (using Gauss elimination with partial pivoting): For each pivot row i,
      - Find pivot with the largest absolute value in column i through swapping operation. 
      - For each row k below the pivot:
       ```
       U[k][j] = U[k][j] - (U[k][i] / U[i][i]) Ã— U[i][j]  (for all j >= i)
       ```
    - Compute Lower Triangular Matrix L**:
      - Set L[i][i] = 1 for all diagonal elements
      - For each element below the diagonal:
     ```
     L[j][i] = (A[j][i] - âˆ‘(L[j][k] Ã— U[k][i])) / U[i][i]
     ```
     where the sum is from k = 0 to i-1

 2. Forward Substitution (Solve Ly = b): For i from 0 to n-1:
   ```
   y[i] = (b[i] - âˆ‘(L[i][j] Ã— y[j])) / L[i][i]
   ```
   where the sum is from j = 0 to i-1

 3. Backward Substitution (Solve Ux = y): For i from n-1 down to 0:
   ```
   x[i] = (y[i] - âˆ‘(U[i][j] Ã— x[j])) / U[i][i]
   ```
   where the sum is from j = i+1 to n-1

 4. Check for Solution Type: Before backward substitution, verify:
    - If U[n-1][n-1] = 0 and y[n-1] != 0: No solution
    - If U[n-1][n-1] = 0 and y[n-1] = 0: Infinite solutions
    - If U[n-1][n-1] != 0: Unique solution
<br>

### ğŸ’» LU-Factorization Method Code

```cpp
#include <bits/stdc++.h>
#include <fstream>
using namespace std;

//-----A=LU : Decomposing the Conefficient Matrix into the product of Lower Triangular and Upper Triangular Matrices-----
void decompose(vector<vector<double>> &a, vector<vector<double>> &u, vector<vector<double>> &l, vector<double> &b, int n)
{
    u = a;

    //-----Finding the Upper Triangular Matrix using Gauss Elimination with Partial Pivoting-----
    for (int i = 0; i < n - 1; i++)
    {
        int pivot = i;
        for (int j = i + 1; j < n; j++)
        {
            if (fabs(u[j][i]) > fabs(u[pivot][i]))
                pivot = j;
        }

        if (pivot != i)
        {
            swap(u[i], u[pivot]);
            swap(a[i], a[pivot]);
            swap(b[i], b[pivot]);
        }

        if (fabs(u[i][i]) < 1e-9)
            continue;

        double x = u[i][i];
        for (int k = i + 1; k < n; k++)
        {
            double y = u[k][i];

            for (int j = i; j < n; j++)
            {
                u[k][j] -= u[i][j] * y / x;
            }
        }
    }

    //------Computing Lower Triangular Matrix-----
    for (int i = 0; i < n; i++)
    {
        l[i][i] = 1;

        for (int j = i + 1; j < n; j++)
        {
            double sum = 0;

            for (int k = 0; k < i; k++)
            {
                sum += l[j][k] * u[k][i];
            }

            if (fabs(u[i][i]) < 1e-9)
                l[j][i] = 0;
            else
                l[j][i] = (a[j][i] - sum) / u[i][i];
        }
    }
}

//-----LUx = b -> Ly = b : Forward Substitution Phase for y which is the solution of the Augmented Matrix [L|b]----
void forward(vector<vector<double>> &l, vector<double> &y, vector<double> &b, int n)
{
    for (int i = 0; i < n; i++)
    {
        double sum = 0;
        for (int j = 0; j < i; j++)
        {
            sum += y[j] * l[i][j];
        }
        y[i] = (b[i] - sum) / l[i][i];
    }
}

//-----Ux = y : Backward Substitution Phase for x which is the solution of the Augmented Matrix [U|y]-----
void backward(vector<vector<double>> &u, vector<double> &x, vector<double> &y, int n)
{
    for (int i = n - 1; i >= 0; i--)
    {
        double sum = 0;
        for (int j = i + 1; j < n; j++)
        {
            sum += x[j] * u[i][j];
        }
        x[i] = (y[i] - sum) / u[i][i];
    }
}

//------Ax = b : Printing the Linear System-------
void systemPrint(vector<vector<double>> &a, vector<double> &b, ofstream &fout)
{
    fout << "The Linear System: \n";
    int n = a.size();
    for (int i = 0; i < n; i++)
    {
        fout << a[i][0] << "*x" << 1 << " ";
        for (int j = 1; j < n; j++)
        {
            if (a[i][j] >= 0)
                fout << " + ";
            fout << a[i][j] << "*x" << j + 1 << " ";
        }
        fout << " = " << b[i] << '\n';
    }
}

void solve(ifstream &fin, ofstream &fout)
{
    //------Reading the number of linear equations of the linear system------
    int n;
    fin >> n;

    vector<vector<double>> a(n, vector<double>(n));    // a is the coefficient matrix
    vector<vector<double>> l(n, vector<double>(n, 0)); // l is the lower triangular matrix
    vector<vector<double>> u(n, vector<double>(n, 0)); // u is the upper triangular matrix
    vector<double> b(n);                               // b is the constants matrix

    //-----Reading the Augmented Matrix into the Coefficient and Constant Matrices------
    for (int i = 0; i < n; i++)
    {
        for (int j = 0; j < n; j++)
        {
            fin >> a[i][j];
            if (i == j)
                l[i][j] = 1;
        }
        fin >> b[i];
    }

    systemPrint(a, b, fout);

    decompose(a, u, l, b, n);

    vector<double> y(n, 0); // y=Ux;
    forward(l, y, b, n);

    //-----Handling the solution types of the system-----
    if (u[n - 1][n - 1] == 0)
    {
        if (y[n - 1])
        {
            fout << "\nThe system has NO SOLUTION.\n";
        }
        else
        {
            fout << "\nThe system has INFINITE SOLUTIONS.\n";
        }
    }

    else
    {
        fout << "\nThe system has UNIQUE SOLUTION. \n";

        vector<double> x(n, 0);
        backward(u, x, y, n);

        //------Printing the solutions of the System------
        fout << "\nSolutions: \n";
        for (int i = 0; i < n; i++)
        {
            fout << "x" << i + 1 << " = " << x[i] << '\n';
        }
    }
}

int main()
{
    ifstream fin("input.txt");
    ofstream fout("output.txt");

    fout << "Solution of Linear System using LU Decomposition Method\nMultiple Testcases\n\n";

    if (!fin)
    {
        fout << "File not found.\n";
        return 0;
    }

    //------Mutiple Test Cases------
    int t;
    fin >> t;

    int cse = 1;

    while (t--)
    {
        for (int i = 0; i < 45; i++)
            fout << "=";
        fout << '\n';
        fout << "Test case: " << cse << "\n";
        solve(fin, fout);
        cse++;
    }
}
```
<br>

### ğŸ“ LU-Factorization Method Input
```
4
3
3 6 1 16
2 4 3 13
1 3 2 9
2
1 1 2
2 2 4
2
1 1 2
2 2 5
5
2 1 -1 3 2 9
1 3 2 -1 1 8
3 2 4 1 -2 20
2 1 3 2 1 17
1 -1 2 3 4 15
```
<br>

### ğŸ“¤ LU-Factorization Method Output
```
Solution of Linear System using LU Decomposition Method
Multiple Testcases

=============================================
Test case: 1
The Linear System: 
3*x1  + 6*x2  + 1*x3  = 16
2*x1  + 4*x2  + 3*x3  = 13
1*x1  + 3*x2  + 2*x3  = 9

The system has UNIQUE SOLUTION. 

Solutions: 
x1 = 1
x2 = 2
x3 = 1
=============================================
Test case: 2
The Linear System: 
1*x1  + 1*x2  = 2
2*x1  + 2*x2  = 4

The system has INFINITE SOLUTIONS.
=============================================
Test case: 3
The Linear System: 
1*x1  + 1*x2  = 2
2*x1  + 2*x2  = 5

The system has NO SOLUTION.
=============================================
Test case: 4
The Linear System: 
2*x1  + 1*x2 -1*x3  + 3*x4  + 2*x5  = 9
1*x1  + 3*x2  + 2*x3 -1*x4  + 1*x5  = 8
3*x1  + 2*x2  + 4*x3  + 1*x4 -2*x5  = 20
2*x1  + 1*x2  + 3*x3  + 2*x4  + 1*x5  = 17
1*x1 -1*x2  + 2*x3  + 3*x4  + 4*x5  = 15

The system has UNIQUE SOLUTION. 

Solutions: 
x1 = 5.15385
x2 = -1
x3 = 2.26154
x4 = -0.138462
x5 = 1.18462
```
<br>

### ğŸ¯ Accuracy Consideration

- Partial pivoting helps avoid division by zero and minimize round-off errors.

- The decomposition process can accumulate round-off errors.

- Using tolerance values (e.g., 1e-9) helps identify singular or near-singular matrices
<br>

### â• Advantages

- Highly efficient when solving multiple systems with the same coefficient matrix but different constant vectors

- The decomposition needs to be computed only once.

- More efficient than Gauss elimination when solving Ax = b for multiple b vectors

- Forms the basis for many advanced numerical algorithms
<br>

### â– Disadvantages

- Requires additional storage for both L and U matrices.

- More complex to implement than simple Gauss elimination.

- Not advantageous if solving only a single system with one right-hand side.

- Requires the coefficient matrix to be square.
<br>

### ğŸš€ Applications

- Solving multiple linear systems with the same coefficient matrix

- Computing matrix determinants and inverses efficiently

- Numerical optimization algorithms requiring repeated linear system solutions
<br>

[Back to Top](#-Table-of-Contents)

---

## Matrix Inversion Method

<br>

### ğŸ“– Matrix Inversion Method Theory

Matrix inversion method is a direct method to solve systems of linear equations by finding the inverse of the coefficient matrix. For a system `Ax = b`, if `A` is invertible (non-singular), the solution can be obtained by multiplying both sides by the inverse matrix `Aâ»Â¹`, giving `x = Aâ»Â¹b`.

The inverse of a matrix `A` is denoted as `Aâ»Â¹` and satisfies the property `AAâ»Â¹ = Aâ»Â¹A = I`, where I is the identity matrix. The method uses Gauss-Jordan elimination to transform the augmented matrix `[A|I]` into `[I|Aâ»Â¹]`.

A matrix has an inverse if and only if its determinant is non-zero. If `det(A) = 0`, the matrix is singular and the system either has no solution or infinitely many solutions.

<br>

### ğŸ¤– Algorithm

 1. Check Determinant: Calculate `det(A)` using Gauss elimination on a copy of `A`. If `det(A) = 0`, the matrix is singular and the system has no unique solution.

 2. Create Augmented Matrix: Create augmented matrix `[A|I]` where `I` is the identity matrix of the same size as `A`.

 3. Apply Gauss-Jordan Elimination:
    - Perform partial pivoting for to find the largest absolute values as pivot
    - For each row `i`, make the diagonal element equal to `1` by dividing the entire row by `A[i][i]`
    - Eliminate all other variables from rows both above and below the pivot using row operations
    - Apply the same operations to the identity matrix portion

 4. Extract Inverse: After completion, the left side becomes `I` and the right side becomes `Aâ»Â¹` resulting in `[I|Aâ»Â¹]`.

 5. Matrix Multiplication: Compute `x = Aâ»Â¹b` by multiplying the inverse matrix with the constant vector.

<br>

### ğŸ’» Matrix Inversion Method Code

```cpp
#include <bits/stdc++.h>
#include <fstream>
using namespace std;

//-----Using Gauss Elimination Method to find the determinant------
double determinant(vector<vector<double>> a, int n)
{
    vector<vector<double>> cof = a;
    for (int i = 0; i < n - 1; i++)
    {
        int pivot = i;
        for (int j = i + 1; j < n; j++)
        {
            if (fabs(cof[j][i]) > fabs(cof[pivot][i]))
                pivot = j;
        }

        if (pivot != i)
            swap(cof[i], cof[pivot]);

        if (fabs(cof[i][i]) < 1e-9)
            continue;

        double x = cof[i][i];

        for (int k = i + 1; k < n; k++)
        {
            double y = cof[k][i];
            for (int j = i; j < n; j++)
            {
                cof[k][j] = cof[k][j] - cof[i][j] * y / x;
            }
        }
    }

    //-----Determinant is the multiplication of the elements along the diagonal of the upper triangular matrix-----
    double det = 1;
    for (int i = 0; i < n; i++)
        det *= cof[i][i];

    return det;
}

//-----Using Gauss Jordan on A|I to find (A^-1)-----
vector<vector<double>> jordan(vector<vector<double>> &a, int n)
{
    vector<vector<double>> id(n, vector<double>(n, 0));
    for (int i = 0; i < n; i++)
        id[i][i] = 1; // identity matrix

    for (int i = 0; i < n; i++)
    {
        int pivot = i;
        for (int j = i + 1; j < n; j++)
        {
            if (fabs(a[j][i]) > fabs(a[pivot][i]))
                pivot = j;
        }
        if (pivot != i)
        {
            swap(a[i], a[pivot]);
            swap(id[i], id[pivot]);
        }

        double x = a[i][i];
        for (int j = i; j < n; j++)
            a[i][j] /= x;
        for (int j = 0; j < n; j++)
            id[i][j] /= x;

        for (int k = 0; k < n; k++)
        {
            if (k != i && a[k][i])
            {
                double y = a[k][i];
                for (int j = 0; j < n; j++)
                {
                    a[k][j] -= a[i][j] * y;
                }
                for (int j = 0; j < n; j++)
                {
                    id[k][j] -= id[i][j] * y;
                }
            }
        }
    }
    return id;
}

vector<double> multiply(vector<vector<double>> inv, vector<double> &b, int n)
{
    vector<double> ans(n, 0);
    for (int i = 0; i < n; i++)
    {
        for (int j = 0; j < n; j++)
        {
            ans[i] += inv[i][j] * b[j];
        }
    }
    return ans;
}

void matrix_inversion(vector<vector<double>> &a, vector<double> &b, int n, ofstream &fout)
{
    if (!determinant(a, n))
    {
        fout << "\nThe system has no or infinetely many solutions.\n";
        return;
    }
    else
    {
        fout << "\nThe system has unique solution.\n";
    }

    vector<vector<double>> inv; 
    inv = jordan(a, n);

    vector<double> x;
    x = multiply(inv, b, n);

    fout << "\nSolutions :\n";
    for (int i = 0; i < n; i++)
    {
        fout << "x" << i + 1 << " = " << x[i] << '\n';
    }
}

//------Ax = b : Printing the Linear System-------
void systemPrint(vector<vector<double>> &a, vector<double> &b, ofstream &fout)
{
    fout << "The Linear System: \n";
    int n = a.size();
    for (int i = 0; i < n; i++)
    {
        fout << a[i][0] << "*x" << 1 << " ";
        for (int j = 1; j < n; j++)
        {
            if (a[i][j] >= 0)
                fout << " + ";
            fout << a[i][j] << "*x" << j + 1 << " ";
        }
        fout << " = " << b[i] << '\n';
    }
}

void solve(istream &fin, ofstream &fout)
{
    int n;
    fin >> n;
    vector<vector<double>> a(n, vector<double>(n));
    vector<double> b(n);

    for (int i = 0; i < n; i++)
    {
        for (int j = 0; j < n; j++)
        {
            fin >> a[i][j];
        }
        fin >> b[i];
    }
    systemPrint(a, b, fout);

    matrix_inversion(a, b, n, fout);
}

int main()
{
    ifstream fin("input.txt");
    ofstream fout("output.txt"); 

    fout << "Solution of Linear System using Matrix Inversion Method\nMultiple Testcases\n\n";

    if (!fin)
    {
        fout << "File not found.\n"; 
        return 0;
    }

    //------Mutiple Test Cases------
    int t;
    fin >> t;

    int cse = 1;

    while (t--)
    {
        for (int i = 0; i < 45; i++)
            fout << "=";
        fout << '\n';
        fout << "Test case: " << cse << "\n";
        solve(fin, fout);
        cse++;
    }
}
```
<br>

### ğŸ“ Matrix Inversion Method Input
```
4
3
3 6 1 16
2 4 3 13
1 3 2 9
2
1 1 2
2 2 4
2
1 1 2
2 2 5
5
2 1 -1 3 2 9
1 3 2 -1 1 8
3 2 4 1 -2 20
2 1 3 2 1 17
1 -1 2 3 4 15
```
<br>

### ğŸ“¤ Matrix Inversion Method Output
```
Solution of Linear System using Matrix Inversion Method
Multiple Testcases

=============================================
Test case: 1
The Linear System: 
3*x1  + 6*x2  + 1*x3  = 16
2*x1  + 4*x2  + 3*x3  = 13
1*x1  + 3*x2  + 2*x3  = 9

The system has unique solution.

Solutions :
x1 = 1
x2 = 2
x3 = 1
=============================================
Test case: 2
The Linear System: 
1*x1  + 1*x2  = 2
2*x1  + 2*x2  = 4

The system has no or infinetely many solutions.
=============================================
Test case: 3
The Linear System: 
1*x1  + 1*x2  = 2
2*x1  + 2*x2  = 5

The system has no or infinetely many solutions.
=============================================
Test case: 4
The Linear System: 
2*x1  + 1*x2 -1*x3  + 3*x4  + 2*x5  = 9
1*x1  + 3*x2  + 2*x3 -1*x4  + 1*x5  = 8
3*x1  + 2*x2  + 4*x3  + 1*x4 -2*x5  = 20
2*x1  + 1*x2  + 3*x3  + 2*x4  + 1*x5  = 17
1*x1 -1*x2  + 2*x3  + 3*x4  + 4*x5  = 15

The system has unique solution.

Solutions :
x1 = 5.15385
x2 = -1
x3 = 2.26154
x4 = -0.138462
x5 = 1.18462
```
<br>

### ğŸ¯ Accuracy Consideration

- Numerical stability depends heavily on the condition number of the matrix

- Partial pivoting is used to reduce round-off errors
 
- The determinant calculation helps identify singular or near-singular matrices
<br>

### â• Advantages

- Provides a direct solution without iteration 

- Useful when solving multiple systems with the same coefficient matrix but different constant vectors

- The inverse matrix can be reused for different right-hand sides
<br>

### â– Disadvantages

- Computationally expensive for large matrices 

- Requires extra memory to store the inverse matrix 

- More prone to round-off errors compared to elimination methods

- Cannot be used for singular matrices (determinant = 0)
<br>

### ğŸš€ Applications

- Solving multiple linear systems with the same coefficient matrix 

- Applied in computer graphics for transformations 
<br>

[Back to Top](#-Table-of-Contents)

---

# Solution of Non-Linear Equations

A non-linear equation is an equation in which the highest power of the variable is greater than 1 or the variables appear in non-linear forms (squared, cubed, trigonometric, logarithmic, etc). <br>
Example:
```
xâ‚‚ + 4xâ‚ = 10
e^x - 4Â·x = 0
```
There are some techniques to solve the non-linear equations. Such as

-> **Bracketing Methods** <br>

-> **Open Methods** <br>


**Bracketing methods** are a class of numerical algorithms used to find the roots of a non-linear equation `f(x) = 0`. They are called **bracketing** methods because they require two initial guesses, a and b, that "bracket" or surround the root between them. <br>

Popular Bracketing Method: <br>

1. **Bisection Method** <br>

2. **False Position Method**<br>

**Open methods** are iterative techniques used to find the roots of a non-linear equation `f(x) = 0` without needing an interval that brackets the root. Open methods typically use a single starting value or two starting values.<br>

Popular Open Method: <br>

1. **Newton-Raphson Method** <br>

2. **Secant Method**<br>

<br>

## Bisection Method

<br>

### ğŸ“– Bisection Method Theory

The Bisection Method is one of the simplest numerical techniques for finding the solution of a non-linear equation `f(x) = 0`. It is also known as binary chopping and half interval method. <br>

The main concept of this method is if the function `f(x)` is real and continuous in the interval `a<x<b` and `f(a)` and `f(b)` are of opposite sign that is , `f(a) *f(b) < 0 `then there exist at least one solution in this interval. <br>

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

**Step 1**: Choose two real numbers `xâ‚` and `xâ‚‚` such that the function changes sign over the interval: `f(xâ‚) * f(xâ‚‚) < 0`<br>

**Step 2**: Calculate the midpoint `xâ‚€` of the current interval: `xâ‚€ = (xâ‚ + xâ‚‚)/2`<br>

**Step 3**: Evaluate the function at the midpoint: `f(xâ‚€)`.<br>

**Step 4**: If `f(xâ‚€) = 0`, then `xâ‚€` is the exact root. And Stop the iteration.<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If `f(xâ‚€) * f(xâ‚‚) < 0`, the root lies between `xâ‚` and `xâ‚€`. Set `xâ‚‚ = xâ‚€`.<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If `f(xâ‚€) * f(xâ‚‚) < 0`, the root lies between `xâ‚€` and `xâ‚‚`. Set `xâ‚ = xâ‚€`.<br>

**Step 5**: Repeat Steps 2 through 4 until the relative error satisfies the stopping criterion:
```|(xâ‚‚ - xâ‚)/xâ‚‚| < e``` <br>

<br>

### ğŸ’» Bisection Method Code

```cpp
#include<bits/stdc++.h>
using namespace std;

void prnt() {
cout << "============================================\n";
cout << " Bisection Method\n";
cout << "============================================\n\n";
}

void RR(){
cout << "============================================\n";
cout<<" Result\n";
cout << "============================================\n";
}

void p(){
cout << "============================================\n";
cout<<" The task is completed\n";
cout << "============================================\n";
}

double f(double x, vector<double>& coef){
double res = 0.0;
int n = coef.size();
for(int i=0;i<n;i++){
res += coef[i]*pow(x,n-1-i);
}
return res;

}

void printPolynomial(vector<double> coef){
cout << "\nPolynomial function:\n";

int n = coef.size() - 1;

for (int i = 0; i <= n; i++) {
int power = n - i;

if (coef[i] == 0) continue;

if (i != 0 && coef[i] > 0)
cout << " + ";
else if(i !=0 && coef[i]<0)
cout<<" - ";

if (power == 0)
cout << fabs(coef[i]);
else if (power == 1)
cout << fabs(coef[i]) << "x";
else if(fabs(coef[i]) >1)
cout << fabs(coef[i]) << "x^" << power;
else
cout << "x^" << power;

}

cout << " = 0\n";
return;
}

double bisectionMethod(vector<double>& coef, double L, double R, double e){
double pos = R, neg = L, mid, x;

do{
mid = (pos+neg)/2.0;
x = f(mid,coef);
if(f(neg,coef)*x<0.0){pos = mid;}
else
neg = mid;
}while(fabs(x)>e);

return mid;
}

int main(){

double x,pos,neg,e = 0.001,mid;
freopen("input.txt", "r", stdin);
freopen("output.txt", "w", stdout);
prnt();
int n;

cin>>n;

vector<double>coef(n+1);


for (int i = 0; i <= n; i++) {
cin >> coef[i];
}
double L, R, h;

cin >> L >> R;


cin >> h;

double x1 = L, x2 = L + h;
bool found = false;

vector<double> roots;
while (x2 <= R) {
if (f(x1, coef) * f(x2, coef) < 0) {
pos = x2;
neg = x1;
roots.push_back(bisectionMethod(coef, x1, x2, e));
x1 = x2;
x2 += h;
continue;

}
x1 = x2;
x2 += h;
}

printPolynomial(coef);

RR();
cout << "\nRoots found in the given range:\n";int i = 0;

for (double root : roots) {
i++;
cout <<"\tx"<<i<<" = "<< fixed << setprecision(6) << root << endl;
}
p();
return 0;
}

```
<br>

### ğŸ“ Bisection Method Input
```
6
1 -21 175 -735 1624 -1764 720
-5 10
0.1

```
<br>

### ğŸ“¤ Bisection Method Output
```
============================================
Bisection Method
============================================


Polynomial function:
x^6 - 21x^5 + 175x^4 - 735x^3 + 1624x^2 - 1764x + 720 = 0
============================================
Result
============================================

Roots found in the given range:
x1 = 1.000006
x2 = 2.000024
x3 = 2.999951
x4 = 4.000049
x5 = 4.999976
x6 = 6.000006
============================================
The task is completed
============================================

```
<br>

### ğŸ¯ Accuracy Consideration

1. Choose a small tolerance `E` as the stopping criterion.<br>

2. Ensure `f(xâ‚) * f(xâ‚‚) < 0` before starting to guarantee a root exists within the interval.<br>

3. Avoid using very large initial intervals to minimize unnecessary computations and potential overflow.<br>

4. The function must be continuous within the closed interval `[xâ‚,xâ‚‚]` for the method to be valid.<br>

5. Stop the algorithm only when the interval width or the relative error is sufficiently small.<br>

<br>

### â• Advantages

1. This method is Simple and Easy to Understand.<br>

2. If the function changes sign over `[a, b]`, the method always converges to a root.<br>

3. Works with only functional value not the differential value. <br>

4. Bisection method Works on Any Continuous Function<br>

5. After n iterations, the maximum error is (b-a)/2<sup>n</sup><br>
<br>

### â– Disadvantages

1. Bisection method takes many iterations. Thatâ€™s why this method is slower than Newton-Raphson Method. <br>

2. Bisection method only works when `f(a) * f(b)<0.`It Fails when both endpoints have same sign. <br>

3. If function touches the axis without crossing the axis, bisection may unable to solve it. <br>

4. This method Can detect only one root in an interval. <br>

<br>

### ğŸš€ Applications

1. Finding roots of transcendental and algebraic equations where analytical solutions are unavailable.<br>

2. Calculating critical values in thermodynamics, fluid dynamics.<br>

3. Solving for balance or equilibrium states in systems influenced by non-linear physical forces.<br>

4. Finding the highest or lowest points of a curve by locating the roots of the function's derivative.<br>
<br>

[Back to Top](#-Table-of-Contents)

---

## False Position Method

<br>

### ğŸ“– False Position Method Theory

The False Position Method, also called Regula Falsi, is a numerical technique used to find the real root of a nonlinear equation
f(x) = 0.It is a bracketing method, meaning it starts with two initial points $x_1$ and $x_2$ such that: f($x_1$) * f($x_2$) < 0.This indicates that the function changes sign and a root lies between them. The formula for this method is : <br>

```xâ‚€ = xâ‚ - f(xâ‚) * (xâ‚‚ - xâ‚)(f(xâ‚‚) - f(xâ‚)```<br>



<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

**Step 1**: Choose two real numbers `xâ‚` and `xâ‚‚` such that the function changes sign over the interval: `f(xâ‚) * f(xâ‚‚) < 0` and Define a stopping criterion $E$.<br>

**Step 2**: Calculate the estimate for the root $x_0$ using the interpolation formula: <br>
```xâ‚€ = xâ‚ - f(xâ‚) * (xâ‚‚ - xâ‚)(f(xâ‚‚) - f(xâ‚)```<br>

**Step 3**: Find the value of the function at the new estimate: `f(xâ‚€)`.<br>

**Step 4**: If `f(xâ‚€) = 0`, then the root is `xâ‚€` and Stop the iteration.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If `f(xâ‚€) * f(xâ‚) < 0`, then set `xâ‚‚ = xâ‚€`.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If `f(xâ‚€) * f(xâ‚‚) < 0`, then set `xâ‚ = xâ‚€`.<br>

**Step 5**: Return to Step 2 and repeat until the relative error satisfies the stopping criterion:<br>
```abs(xâ‚‚ - xâ‚)/xâ‚‚) < E```
<br>

### ğŸ’» False Position Method Code

```cpp
#include<bits/stdc++.h>
using namespace std;

void prnt() {
cout << "============================================\n";
cout << " False Position Method\n";
cout << "============================================\n\n";
}

void RR(){
cout << "============================================\n";
cout<<" Result\n";
cout << "============================================\n";
}

void p(){
cout << "============================================\n";
cout<<" The task is completed\n";
cout << "============================================\n";
}

double f(double x, vector<double>& coef){
double res = 0.0;
int n = coef.size();
for(int i=0;i<n;i++){
res += coef[i]*pow(x,n-1-i);
}
return res;

}

void printPolynomial(vector<double> &coef){
cout << "\nPolynomial function:\n";

int n = coef.size() - 1;

for (int i = 0; i <= n; i++) {
int power = n - i;

if (coef[i] == 0) continue;

if (i != 0 && coef[i] > 0)
cout << " + ";
else if(i !=0 && coef[i]<0)
cout<<" - ";

if (power == 0)
cout << fabs(coef[i]);
else if (power == 1)
cout << fabs(coef[i]) << "x";
else if(fabs(coef[i]) >1)
cout << fabs(coef[i]) << "x^" << power;
else
cout << "x^" << power;

}

cout << " = 0\n";
return;
}

float false_position(vector<double>& coef,double x1, double x2, double e){
double x0,fx0;
do{
x0 = x1 - f(x1, coef) * (x2 - x1) /
(f(x2, coef) - f(x1, coef));

fx0 = f(x0,coef);
if(fx0 == 0.0) return x0;

else if(f(x1,coef)*fx0<0.0) x2 = x0;
else x2 = x0;
}while(abs(fx0)>e);

return x0;
}

int main(){



double x,pos,neg,e = 0.001,mid;
freopen("input.txt", "r", stdin);
freopen("output.txt", "w", stdout);
prnt();
int n;
cin>>n;

vector<double>coef(n+1);


for (int i = 0; i <= n; i++) {
cin >> coef[i];
}
double L, R, h;

cin >> L >> R;

cin >> h;

double x1 = L, x2 = L + h;

vector<double> roots;
while (x2 <= R) {
if (f(x1, coef) * f(x2, coef) < 0) {
pos = x2;
neg = x1;
roots.push_back(false_position(coef, x1, x2, e));
x1 = x2;
x2 += h;
continue;

}
x1 = x2;
x2 += h;
}

printPolynomial(coef);


RR();
cout << "\nRoots found in the given range:\n";int i = 0;
for (double root : roots) {
i++;
cout <<"\tx"<<i<<" = "<< fixed << setprecision(6) << root << endl;
}
p();

return 0;
}



```
<br>

### ğŸ“ False Position Method Input
```
6
1 -21 175 -735 1624 -1764 720
-5 10
0.1

```
<br>

### ğŸ“¤ False Position Method Output
```
============================================
False Position Method
============================================


Polynomial function:
x^6 - 21x^5 + 175x^4 - 735x^3 + 1624x^2 - 1764x + 720 = 0
============================================
Result
============================================

Roots found in the given range:
x1 = 1.000000
x2 = 2.000000
x3 = 3.000000
x4 = 4.000000
x5 = 5.000000
x6 = 6.000000
============================================
The task is completed
============================================
```
<br>

### ğŸ¯ Accuracy Consideration

1. Choose a sufficiently small tolerance ğ¸.<br>

2. Ensure the initial interval satisfies `f(a) * f(b) < 0`.<br>

3. Need to use double-precision floating values.<br>

4. Check sign change carefully at each step.<br>

5. Ensure the function is continuous in the interval.<br>

<br>

### â• Advantages

1. Guaranteed convergence if f`(a) * f(b) < 0`.<br>

2. The algorithm always keeps the root trapped within the interval, providing high reliability.<br>

3. Faster than the Bisection method in many cases.<br>

4. The medthod is Simple and easy to implement.<br>

5. No derivative needed.

<br>

### â– Disadvantages

1. Depending on the curve of the function, convergence can become slow.<br>

2. One endpoint may remain fixed for many iterations.<br>

3. If the function values are not well-balanced across the interval, the process can stagnate.<br>

4. For certain non-linear shapes, it may still require a high number of iterations to achieve extreme precision.<br>

<br>

### ğŸš€ Applications

1. Solving nonlinear algebraic equations.<br>

2. Finding roots in engineering design problems.<br>

3. Solving for equilibrium points in systems involving non-linear forces or complex potential energy surfaces.<br>

4. Identifying the local extrema of a function by finding the roots of its first derivative.<br>
<br>

[Back to Top](#-Table-of-Contents)

---

## Newton-Raphson Method

<br>

### ğŸ“– Newton-Raphson Method Theory

The Newton-Raphson Method is a high-speed numerical technique used to find the root of a nonlinear equation `f(x) = 0`.<br>

The process begins with an initial guess, `xâ‚€`.A tangent is drawn to the curve at that point to see where it crosses the x-axis.That crossing point becomes the next guess, repeating until the values move closer to the actual root.<br>

The formula :<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x<sub>n+1</sub>â€‹ = x<sub>n</sub> â€‹âˆ’ f<sup>â€²</sup>(x<sub>n</sub>â€‹)/f(x<sub>n</sub>â€‹)<br>

The formula uses both the function value and its derivative so each step is guided by the slope of the curve.If the initial guess is close and the derivative is not zero, the accuracy nearly doubles with every step.<br>â€‹

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

**Step 1**: Find f<sup>'</sup>($x_n$).<br>

**Step 2**: Choose 2 real numbers a and b such that `f(a) * f(b) < 0` and stopping criterion E. If such `a` and `b` do not exist, then randomly guess a.<br>

**Step 3**: Assume `xâ‚€ = a`.<br>

**Step 4**: Find x<sub>n+1</sub>.<br>

**Step 5**: If |x<sub>n+1</sub> - $x_n$| < E, then the root is x<sub>n+1</sub> and stop. Otherwise, set $x_n$ = x<sub>n+1</sub> and repeat the iteration.<br>

<br>

### ğŸ’» Newton-Raphson Method Code

```cpp
#include <bits/stdc++.h>
using namespace std;

void prnt() {
cout << "============================================\n";
cout << " Newton Rapshon Method\n";
cout << "============================================\n\n";
}

void RR(){
cout << "============================================\n";
cout<<" Result\n";
cout << "============================================\n";
}

void p(){
cout << "============================================\n";
cout<<" The task is completed\n";
cout << "============================================\n";
}

double f(double x, vector<double>& coef){
double res = 0.0;
int n = coef.size();
for(int i=0;i<n;i++){
res += coef[i]*pow(x,n-1-i);
}
return res;

}

void printPolynomial(vector<double> &coef){
cout << "\nPolynomial function:\n";

int n = coef.size() - 1;

for (int i = 0; i <= n; i++) {
int power = n - i;

if (coef[i] == 0) continue;
if (i != 0 && coef[i] > 0)
cout << " + ";
else if(i !=0 && coef[i]<0)
cout<<" - ";
if (power == 0)
cout << fabs(coef[i]);
else if (power == 1)
cout << fabs(coef[i]) << "x";
else if(fabs(coef[i]) >1)
cout << fabs(coef[i]) << "x^" << power;
else
cout << "x^" << power;

}

cout << " = 0\n";
return;
}

double differentiate(vector<double>coef, double x) {
double h = 1e-6;
return (f(x + h,coef) - f(x - h,coef)) / (2 * h);
}

double newton_raphson(vector<double>coef, double initial_guess, double e) {
double x0 = initial_guess;
double x1;
while (true) {
double f0 = f(x0, coef);
double df0 = differentiate(coef, x0);
x1 = x0 - f0 / df0;
if (fabs(f(x1, coef)) < e) {
break;
}
x0 = x1;
}
return x1;
}

int main() {
double x,pos,neg,e = 0.001,mid;
freopen("input.txt", "r", stdin);
freopen("output.txt", "w", stdout);
prnt();
int n;
cin>>n;

vector<double>coef(n+1);


for (int i = 0; i <= n; i++) {
cin >> coef[i];
}
double L, R, h;

cin >> L >> R;

cin >> h;

double x1 = L, x2 = L + h;

vector<double> roots;
while (x2 <= R) {
if (f(x1, coef) * f(x2, coef) < 0) {
pos = x2;
neg = x1;
roots.push_back(newton_raphson(coef,x2,e));
x1 = x2;
x2 += h;
continue;

}
x1 = x2;
x2 += h;
}

printPolynomial(coef);


RR();
cout << "\nRoots found in the given range:\n";int i = 0;
for (double root : roots) {
i++;
cout <<"\tx"<<i<<" = "<< fixed << setprecision(6) << root << endl;
}
p();
return 0;
}


```
<br>

### ğŸ“ Newton-Raphson Method Input
```
6
1 -21 175 -735 1624 -1764 720
-5 10
0.1

```
<br>

### ğŸ“¤ Newton-Raphson Method Output
```
============================================
Newton Rapshon Method
============================================


Polynomial function:
x^6 - 21x^5 + 175x^4 - 735x^3 + 1624x^2 - 1764x + 720 = 0
============================================
Result
============================================

Roots found in the given range:
x1 = 1.000000
x2 = 2.000000
x3 = 3.000000
x4 = 4.000000
x5 = 5.000000
x6 = 6.000001
============================================
The task is completed
============================================

```
<br>

### ğŸ¯ Accuracy Consideration

1. Choose a good initial guess close to the actual root.<br>

2. Ensure f<sup>'</sup>($x_n$) â‰  0.<br>

3. Need to use double-precision floating values.<br>

4. Have to Ensure the function is differentiable in the interval.<br>

5. Use enough iterations to reach desired accuracy.<br>

<br>

### â• Advantages

1. It features quadratic convergence near the root, meaning the number of correct digits roughly doubles with each step.<br>

2. Unlike bracketing methods, it requires only one initial guess to begin the iteration.<br>

3. The method can be adapted to handle and find complex roots.<br>

4. The method has Simple iterative formula.<br>

<br>

### â– Disadvantages

1. This method Requires derivative of the function.<br>

2. The process may fail to find a root if the initial guess is placed too far from the actual root.<br>

3. The algorithm fails if the derivative becomes zero at any iteration.<br>

4. Depending on the function's shape, the method can converge to the wrong root or enter an infinite oscillation loop between two points.<br>

<br>

### ğŸš€ Applications

1. Finding roots of nonlinear algebraic equations.<br>

2. Can be able to solve Solving transcendental equations.<br>

3. Calculating critical tolerances and dimensions in mechanical and structural design calculations.<br>

4. Computing intersection points and rendering complex curves for animations and 3D modeling.<br>

5. Solving stability equations and processing signals in automated control applications.<br>
<br>

[Back to Top](#-Table-of-Contents)

---

## Secant Method

<br>

### ğŸ“– Secant Method Theory

The Secant Method is an iterative numerical technique designed to find the roots of a nonlinear equation `f(x) = 0`.It approximates the root by drawing a secant line through two points on the function's curve.Unlike the Newton-Raphson method, it does not require the evaluation of a derivative, making it computationally simpler.Each new approximation is found where the secant line intersects the x-axis, using the formula: <br>

```xâ‚™â‚Šâ‚ = xâ‚™ âˆ’ f(xâ‚™) Â· (xâ‚™ âˆ’ xâ‚™â‚‹â‚) / ( f(xâ‚™) âˆ’ f(xâ‚™â‚‹â‚) )```

It converges faster than the Bisection method but slightly slower than the Newton-Raphson method.The method is highly effective when the initial guesses are chosen close to the actual root.<br>

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

**Step 1**: Define the polynomial function `f(x)` using the given coefficients and specify the tolerance E (stopping criterion) and the maximum number of iterations.<br>

**Step 2**: Identify two initial points, `xâ‚€` and `xâ‚`. In your code, these are determined by scanning the range `[L, R]`with step h until a sign change is found `f(xâ‚) * f(xâ‚‚) < 0`.<br>

**Step 3**: Calculate the next approximation $x_2$ using the secant formula:<br>
```xâ‚‚ = xâ‚ âˆ’ f(xâ‚) Â· (xâ‚ âˆ’ xâ‚€) / ( f(xâ‚) âˆ’ f(xâ‚€) )```

<br>

**Step 4**: Check if the absolute difference between the last two approximations is less than the tolerance: `abs(xâ‚‚ -xâ‚) < E ` If true, the root is $x_2$. And Stop the iteration.

**Step 5**: If the stopping criterion is not met:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set `xâ‚€ = xâ‚`.Set `xâ‚ = xâ‚‚`.<br>
Repeat from Step 3 until the root is found or the maximum iterations are reached.

<br>

### ğŸ’» Secant Method Code

```cpp
#include <bits/stdc++.h>
using namespace std;

void prnt() {
cout << "============================================\n";
cout << " Secant Method\n";
cout << "============================================\n\n";
}

void RR(){
cout << "============================================\n";
cout<<" Result\n";
cout << "============================================\n";
}

void p(){
cout << "============================================\n";
cout<<" The task is completed\n";
cout << "============================================\n";
}

double f(double x, vector<double>& coef){
double res = 0.0;
int n = coef.size();
for(int i = 0; i < n; i++){
res += coef[i] * pow(x, n - 1 - i);
}
return res;
}

void printPolynomial(vector<double> &coef){
cout << "\nPolynomial function:\n";
int n = coef.size() - 1;

for (int i = 0; i <= n; i++) {
int power = n - i;
if (coef[i] == 0) continue;

if (i != 0 && coef[i] > 0) cout << " + ";
else if (i != 0 && coef[i] < 0) cout << " - ";

if (power == 0)
cout << fabs(coef[i]);
else if (power == 1)
cout << fabs(coef[i]) << "x";
else if (fabs(coef[i]) > 1)
cout << fabs(coef[i]) << "x^" << power;
else
cout << "x^" << power;
}
cout << " = 0\n";
}

double secant(vector<double> coef, double x0, double x1, double e, int maxIter) {
double x2;
for (int iter = 1; iter <= maxIter; iter++) {
double f0 = f(x0, coef);
double f1 = f(x1, coef);

if (fabs(f1 - f0) < 1e-12) {
cout << "Denominator too small. Method fails.\n";
return x1;
}

x2 = x1 - f1 * (x1 - x0) / (f1 - f0);

if (fabs(x2 - x1) < e)
return x2;

x0 = x1;
x1 = x2;
}
return x1;
}

int main() {
freopen("input.txt", "r", stdin);
freopen("output.txt", "w", stdout);

prnt();

int n;
cin >> n;

vector<double> coef(n + 1);
for (int i = 0; i <= n; i++) {
cin >> coef[i];
}

double L, R, h;
cin >> L >> R;
cin >> h;

double e = 1e-6;
int maxIter = 100;

vector<double> roots;

double x1 = L, x2 = L + h;
while (x2 <= R) {
if (f(x1, coef) * f(x2, coef) < 0) {
roots.push_back(secant(coef, x1, x2, e, maxIter));
}
x1 = x2;
x2 += h;
}

printPolynomial(coef);

RR();
cout << "\nRoots found in the given range:\n";
int i = 0;
for (double root : roots) {
cout << "\tx" << ++i << " = "
<< fixed << setprecision(6) << root << endl;
}
p();

return 0;
}

```
<br>

### ğŸ“ Secant Method Input
```
6
1 -21 175 -735 1624 -1764 720
-5 10
0.1

```
<br>

### ğŸ“¤ Secant Method Output
```
============================================
Secant Method
============================================


Polynomial function:
x^6 - 21x^5 + 175x^4 - 735x^3 + 1624x^2 - 1764x + 720 = 0
============================================
Result
============================================

Roots found in the given range:
x1 = 1.000000
x2 = 2.000000
x3 = 3.000000
x4 = 4.000000
x5 = 5.000000
x6 = 6.000000
============================================
The task is completed
============================================

```
<br>

### ğŸ¯ Accuracy Consideration

1. The accuracy of the Secant Method depends strongly on the choice of the initial approximations `xâ‚` and `xâ‚€`.<br>

2. It converges faster than the bisection method but slower than the Newtonâ€“Raphson method.<br>

3. If the initial guesses are far from the actual root, convergence may be slow or may fail.<br>

4. Accuracy can be affected when `f(xâ‚) - f(xâ‚€)`is very small, leading to numerical instability.<br>

<br>

### â• Advantages

1. This method does not require calculation of derivatives.<br>

2. Faster convergence than the bisection method.<br>

3. Secant method is very simple and esay to implement.<br>

4. This method is more efficient than fixed-point iteration.<br>

<br>

### â– Disadvantages

1. The method requires two initial guesses.<br>

2. Secant method is slower than Newtonâ€“Raphson method.<br>

3. Convergence is not guaranteed for poor initial guesses.<br>

4. Can fail if `f(xâ‚) - f(xâ‚€)` becomes very small.<br>

<br>

### ğŸš€ Applications

1. Finding roots of nonlinear algebraic equations.<br>

2. Solving transcendental equations.<br>

3. Can solve the numerical methods in computer science.<br>

4. Can able to solve physics and applied mathematics problems.<br>
<br>

[Back to Top](#-Table-of-Contents)

---



# Curve-Fitting

Curve fitting, also known as regression analysis, is a fundamental numerical method used to find a mathematical function that best approximates the relationship between variables in a set of experimental or observational data points. The goal is to construct a continuous curve or function that passes through or near the given data points in a way that minimizes the overall error.

The most common approach to curve fitting is the Least Square Method. This method finds the curve that minimizes the sum of the squares of the residuals (vertical distances between data points and the fitted curve).

For data points `(xâ‚, yâ‚), (xâ‚‚, yâ‚‚), ..., (xâ‚™, yâ‚™)` and a fitting function `f(x)`, the least squares method minimizes:

```
S = Î£[yáµ¢ - f(xáµ¢)]Â²
```

where `S` is the sum of squared residuals.

Curve fitting can be broadly classified based on the type of function being fitted:

  1. Linear Regression (First-Order Polynomial): Fits a straight line: `y = ax + b`

  2. Polynomial Regression (Higher-Order Polynomials)**: Fits polynomials: `y = aâ‚€ + aâ‚x + aâ‚‚xÂ² + ... + aâ‚˜xáµ`

  3. Transcendental (Non-Polynomial) Regression: Fits exponential, power, logarithmic, or other non-polynomial functions.

<br>


## Least Square Regression (Linear Equation)

<br>

### ğŸ“– Least Square Regression (Linear Equation) Theory

Least square regression is a statistical method used to find the best-fitting straight line through a set of data points. The method minimizes the sum of the squares of the vertical deviations from each data point to the line.

For a linear equation of the form `y = ax + b`, the least square method finds the values of coefficients a and b that minimize the sum of squared errors: 
```
Î£(yáµ¢ - (axáµ¢ + b))Â²
```

The best-fit line is determined by calculating the slope (b) and y-intercept (a) using the normal equations derived from partial derivatives of the error function. This method is also known as linear regression.
<br>

### ğŸ”¢ Mathematical Representation

For `n` data points `(xâ‚, yâ‚), (xâ‚‚, yâ‚‚), ..., (xâ‚™, yâ‚™)`, the linear equation is:
```
y = bx + a
```

The coefficients are calculated using:
```
b = (nâˆ‘(xáµ¢yáµ¢) - âˆ‘xáµ¢âˆ‘yáµ¢) / (nâˆ‘(xáµ¢Â²) - (âˆ‘xáµ¢)Â²)

a = (âˆ‘yáµ¢ - bâˆ‘xáµ¢) / n
```

Where:
- `n` is the number of data points
- `âˆ‘xáµ¢` is the sum of all x values
- `âˆ‘yáµ¢` is the sum of all y values
- `âˆ‘(xáµ¢yáµ¢)` is the sum of products of x and y
- `âˆ‘(xáµ¢Â²)` is the sum of squares of x values

<br>

### ğŸ¤– Algorithm

  1. Calculate Summations: 
     - Compute âˆ‘xáµ¢ (sum of x values)
     - Compute âˆ‘yáµ¢ (sum of y values)
     - Compute âˆ‘(xáµ¢yáµ¢) (sum of products)
     - Compute âˆ‘(xáµ¢Â²) (sum of x squares)

  2. Compute Slope (b):
   ```
   b = (nâˆ‘(xáµ¢yáµ¢) - âˆ‘xáµ¢âˆ‘yáµ¢) / (nâˆ‘(xáµ¢Â²) - (âˆ‘xáµ¢)Â²)
   ```

  3. Compute Intercept (a):
   ```
   a = (âˆ‘yáµ¢ - bâˆ‘xáµ¢) / n
   ```

  4. Form the fitted line Equation: `y = bx + a`.

  5. For a given value of x, estimate y using the fitted equation.

<br>

### ğŸ’» Least Square Regression (Linear Equation) Code

```cpp
#include <bits/stdc++.h>
#include <fstream>
using namespace std;

void solve(ifstream &fin, ofstream &fout)
{
    //-----Reading the number of data points----
    int n;
    fin >> n;

    //----Reading the data points in x and y vectors
    vector<double> x(n), y(n);
    for (int i = 0; i < n; i++)
    {
        fin >> x[i] >> y[i];
    }

    double sumX = 0, sumY = 0, sumXY = 0, sumXsq = 0;

    for (int i = 0; i < n; i++)
    {
        sumX += x[i];
        sumY += y[i];
        sumXY += (x[i] * y[i]);
        sumXsq += (x[i] * x[i]);
    }

    double b = (n * sumXY - sumX * sumY) / (n * sumXsq - sumX * sumX);
    double a = (sumY - b * sumX) / n;

    fout << "Linear fitted equation: \n";
    fout << "y = " << b << "x ";
    if (a)
        fout << "+ " << a; //-----to check if the equation went through the origin-----
    fout << "\n";

    int est;
    fin >> est;

    fout << "Estimate the value of y for x = " << est << " : ";
    fout << (b * est + a) << "\n\n";
}

int main()
{
    ifstream fin("input.txt");
    ofstream fout("output.txt");

    fout << "Least Square Regression Method for Linear Equations\nMultiple Testcases\n\n";

    if (!fin)
    {
        fout << "File not found.\n";
        return 0;
    }

    //-----Multiple Testcases----
    int t;
    fin >> t;

    int cse = 1;
    while (t--)
    {
        for (int i = 0; i < 45; i++)
            fout << "=";
        fout << '\n';
        fout << "Test case: " << cse << "\n";
        solve(fin, fout);
        cse++;
    }
}
```
<br>

### ğŸ“ Least Square Regression (Linear Equation) Input
```
3
7
1 3
2 4
3 4
4 5
5 8
6 9
7 10
8
5
0 10
1 8
2 6
3 4
4 2
5
6
0 0
1 2
2 4
3 6
4 8 
5 10
6
```
<br>

### ğŸ“¤ Least Square Regression (Linear Equation) Output
```
Least Square Regression Method for Linear Equations
Multiple Testcases

=============================================
Test case: 1
Linear fitted equation: 
y = 1.25x + 1.14286
Estimate the value of y for x = 8 : 11.1429

=============================================
Test case: 2
Linear fitted equation: 
y = -2x + 10
Estimate the value of y for x = 5 : 0

=============================================
Test case: 3
Linear fitted equation: 
y = 2x 
Estimate the value of y for x = 6 : 12
```
<br>

### ğŸ¯ Accuracy Consideration

- The method assumes a linear relationship between variables

- More data points generally lead to better approximation

- Works best when data points show a linear trend
<br>

### â• Advantages

- Simple and easy to implement

- Computationally efficient with O(n) complexity

- Can be used for prediction and trend analysis
  
<br>

### â– Disadvantages

- Only applicable to linear relationships

- Assumes errors are normally distributed 

- Does not work well for non-linear data patterns

- May not be appropriate for curved relationships
- 
<br>

### ğŸš€ Applications

- Predicting future values based on historical data

- Analyzing trends in economics and business

- Finding relationships between variables in scientific experiments
<br>

[Back to Top](#-Table-of-Contents)

---

## Least Square Regression (Transcendental Equation)

<br>

### ğŸ“– Least Square Regression (Transcendental Equation) Theory

Transcendental equations involve transcendental functions such as exponential, logarithmic, trigonometric etc. These equations cannot be solved algebraically and require numerical methods.

In curve fitting, transcendental regression is used when data follows exponential growth/decay, power laws, or other non-polynomial patterns. The method transforms the transcendental equation into a linear form using logarithms, then applies linear least squares regression.

Common transcendental forms include:
- *Power equation*: `y = ax^b` (transformed: `ln(y) = ln(a) + b * ln(x)`)
- *Exponential equation*: `y = ae^(bx)` (transformed: `ln(y) = ln(a) + bx`)
- *Offset exponential*: `y = a + be^(x/4)` (linear in a and b after transformation)
  
<br>

### ğŸ”¢ Mathematical Representation

**1. Power Equation: `y = ax^b`**

Taking logarithm: 
```
ln(y) = ln(a) + b * ln(x)
```

Let `Y = ln(y)`, `X = ln(x)`, `B = b`, `A = ln(a)`

we get,
```
Y = A + BX (linear form)
```

After solving,
```
b = B, a = e^A
```

**2. Exponential Equation: `y = ae^(bx)`**

Taking logarithm,
```
ln(y) = ln(a) + bx
```
Let `Y = ln(y)`, `B = b`, `A = ln(a)`

we get,
```
Y = A + Bx (linear form)
```
After solving,
```
b = B, a = e^A
```

**3. Offset Exponential: `y = a + be^(x/4)`**

Let `f(x) = e^(x/4)`

we get,
```
 y = a + bÂ·f(x) (linear in a and b)
```
Use linear regression with `y` and `f(x)`

<br>

### ğŸ¤– Algorithm

 1. Transform data according to the equation type 
    - For Power Equation `(y = ax^b)`, transform data: `X = ln(x)`, `Y = ln(y)`
    - For Exponential Equation `(y = ae^(bx))`, transform data: `Y = ln(y)`, keep `x` unchanged
    - For Offset Exponential `(y = a + be^(x/4))`, calculate `f(x) = e^(x/4)` for each data point

 2. Compute necessary sums for linear regression

 3. Calculate coefficients using normal equations. Transform coefficients back to original form if needed

 4. Form the fitted equation. Use equation for estimation

<br>

### ğŸ’» Least Square Regression (Transcendental Equation) Code

```cpp
#include <bits/stdc++.h>
#include <fstream>
using namespace std;

//-----For the power equations of the form: y=ax^b -------
void power(vector<double> lnx, vector<double> lny, double est, ofstream &fout)
{
    int n = lnx.size();
    double sumLNx = 0, sumLNy = 0, sumLNxy = 0, sumSqlnx = 0;
    for (int i = 0; i < n; i++)
    {
        sumLNx += lnx[i];
        sumLNy += lny[i];
        sumLNxy += (lnx[i] * lny[i]);
        sumSqlnx += (lnx[i] * lnx[i]);
    }

    double b = (n * sumLNxy - sumLNx * sumLNy) / (n * sumSqlnx - sumLNx * sumLNx);
    double a = (sumLNy - b * sumLNx) / n;
    a = exp(a);

    fout << "Power Equation: \n";
    fout << "y = " << a << " * x^(" << b << ")\n";

    fout << "Estimate the value of y for x = " << est << " : ";
    fout << a * (pow(est, b)) << "\n\n";
}

//-----For the exponential equations of the form: y=ae^(bx) -------
void exponential(vector<double> x, vector<double> lny, double est, ofstream &fout)
{
    int n = x.size();
    double sumX = 0, sumLNy = 0, sumXsq = 0, sumXlny = 0;
    for (int i = 0; i < n; i++)
    {
        sumX += x[i];
        sumLNy += lny[i];
        sumXlny += (x[i] * lny[i]);
        sumXsq += (x[i] * x[i]);
    }

    double b = (n * sumXlny - sumX * sumLNy) / (n * sumXsq - sumX * sumX);
    double a = (sumLNy - b * sumX) / n;
    a = exp(a);

    fout << "Exponential Equation: \n";
    fout << "y = " << a << " * e^(" << b << "x)\n";

    fout << "Estimate the value of y for x = " << est << " : ";
    fout << a * (exp(b * est)) << "\n\n";
}

//-----For the offset exponential equations of the form: y=a+be^(x/4) -------
void ofset(vector<double> y, vector<double> fx, double est, ofstream &fout)
{
    int n = y.size();
    double sumY = 0, sumfx = 0, sumfxy = 0, sumSqfx = 0;
    for (int i = 0; i < n; i++)
    {
        sumY += y[i];
        sumfx += fx[i];
        sumfxy += (fx[i] * y[i]);
        sumSqfx += (fx[i] * fx[i]);
    }

    double b = (n * sumfxy - sumfx * sumY) / (n * sumSqfx - sumfx * sumfx);
    double a = (sumY - b * sumfx) / n;

    fout << "Offset Exponential Equation: \n";
    fout << "y = " << a << " + " << b << " * e^(x/4)\n";

    fout << "Estimate the value of y for x = " << est << " : ";
    fout << a + b * (exp(est / 4)) << "\n\n";
}

void solve(ifstream &fin, ofstream &fout)
{
    //-----Reading the number of data points------
    int n;
    fin >> n;

    //-----Reading the data points in two vectors-----
    vector<double> x(n), y(n);
    for (int i = 0; i < n; i++)
    {
        fin >> x[i] >> y[i];
    }

    vector<double> lnx(n), lny(n), fx(n);
    for (int i = 0; i < n; i++)
    {
        lnx[i] = log(x[i]);
        lny[i] = log(y[i]);
        fx[i] = exp(x[i] / 4);
    }

    double est;
    fin >> est;

    fout << "Transcendental fitted equations: \n";
    power(lnx, lny, est, fout);

    exponential(x, lny, est, fout);

    ofset(y, fx, est, fout);
}

int main()
{
    ifstream fin("input.txt");
    ofstream fout("output.txt");

    fout << "Least Square Regression Method for Transcendental Equations\nMultiple Testcases\n\n";

    if (!fin)
    {
        fout << "File not found.\n";
        return 0;
    }

    //-----Multiple Testcases----
    int t;
    fin >> t;

    int cse = 1;
    while (t--)
    {
        for (int i = 0; i < 45; i++)
            fout << "=";
        fout << '\n';
        fout << "Test case: " << cse << "\n";
        solve(fin, fout);
        cse++;
    }
}
```
<br>

### ğŸ“ Least Square Regression (Transcendental Equation) Input
```
1
5
1 50
2 80
3 96
4 120
5 145
6
```
<br>

### ğŸ“¤ Least Square Regression (Transcendental Equation) Output
```
Least Square Regression Method for Transcendental Equations
Multiple Testcases

=============================================
Test case: 1
Transcendental fitted equations: 
Power Equation: 
y = 49.884 * x^(0.642116)
Estimate the value of y for x = 6 : 157.625

Exponential Equation: 
y = 43.1231 * e^(0.253489x)
Estimate the value of y for x = 6 : 197.352

Offset Exponential Equation: 
y = 5.74924 + 41.0587 * e^(x/4)
Estimate the value of y for x = 6 : 189.761
```
<br>

### ğŸ¯ Accuracy Consideration

- Logarithmic transformation requires all data points to be positive (y > 0 for ln(y))

- Errors are minimized in the transformed space, not in the original space

- Different equation forms may fit the same data with varying accuracy

<br>

### â• Advantages

- Can model exponential growth and decay patterns effectively

- Linearization technique simplifies the fitting process

- Applicable to a wide variety of non-linear relationships

- Computationally efficient due to linear regression after transformation
  
<br>

### â– Disadvantages

- Requires positive data values for logarithmic transformation

- Limited to specific functional forms (exponential, power, etc.)

<br>

### ğŸš€ Applications

- Population growth and decline modeling 

- Learning curves
<br>

[Back to Top](#-Table-of-Contents)

---

## Least Square Regression (Polynomial Equation)

<br>

### ğŸ“– Least Square Regression (Polynomial Equation) Theory

Polynomial regression is an extension of linear regression where the relationship between the independent variable `x` and dependent variable `y` is modeled as an n-th degree polynomial. This method is used when the data shows a curved pattern that cannot be adequately represented by a straight line.

The least squares method is used to find the coefficients of the polynomial that minimize the sum of squared residuals. For a polynomial of degree `m`: 
```
y = aâ‚€ + aâ‚x + aâ‚‚xÂ² + ... + aâ‚˜xáµ
```
the method determines the coefficients aâ‚€, aâ‚, ..., aâ‚˜.

<br>

### ğŸ”¢ Mathematical Representation

For n data points `(xâ‚, yâ‚), (xâ‚‚, yâ‚‚), ..., (xâ‚™, yâ‚™)`, fit a polynomial of degree `m`:
```
y = aâ‚€ + aâ‚x + aâ‚‚xÂ² + ... + aâ‚˜xáµ
```

The normal equations form a system of (m+1) linear equations:
```
[âˆ‘1      âˆ‘x      âˆ‘xÂ²    ... âˆ‘xáµ    ] [aâ‚€]   [âˆ‘y    ]
[âˆ‘x      âˆ‘xÂ²     âˆ‘xÂ³    ... âˆ‘xáµâºÂ¹  ] [aâ‚]   [âˆ‘xy   ]
[âˆ‘xÂ²     âˆ‘xÂ³     âˆ‘xâ´    ... âˆ‘xáµâºÂ²  ] [aâ‚‚] = [âˆ‘xÂ²y  ]
[...     ...     ...    ... ...    ] [...] = [...   ]
[âˆ‘xáµ     âˆ‘xáµâºÂ¹   âˆ‘xáµâºÂ²  ... âˆ‘xÂ²áµ  ] [aâ‚˜]   [âˆ‘xáµy  ]
```

where all sums are from i=1 to n.

<br>

### ğŸ¤– Algorithm

 1. Read n data points (xáµ¢, yáµ¢) and the degree m of the polynomial.

 2. Compute sums for powers of x from 0 to 2m:
     - âˆ‘xâ° = n
     - âˆ‘xÂ¹, âˆ‘xÂ², ..., âˆ‘xÂ²áµâ»Â²

 3. Compute sums of y multiplied by powers of x:
     - âˆ‘y, âˆ‘xy, âˆ‘xÂ²y, ..., âˆ‘xáµy

 4. Create the (m+1) Ã— (m+2) augmented matrix using the calculated sums.

 5. Use Gauss elimination to solve the system of normal equations to find coefficients aâ‚€, aâ‚, ..., aâ‚˜.

 6. Form the polynomial equation using the calculated coefficients.

 7. Use the polynomial to estimate y values for given x values.

<br>

### ğŸ’» Least Square Regression (Polynomial Equation) Code

```cpp
#include <bits/stdc++.h>
#include <fstream>
using namespace std;

void gauss(vector<double> &ans, vector<vector<double>> cof)
{
    //----farward elimination phase-----
    int n = cof.size();
    for (int i = 0; i < n - 1; i++)
    {
        int pivot = i;
        for (int j = i + 1; j < n; j++)
        {
            if (fabs(cof[j][i]) > fabs(cof[pivot][i]))
                pivot = j;
        }

        if (pivot != i)
            swap(cof[i], cof[pivot]);

        if (fabs(cof[i][i]) < 1e-9)
            continue;

        double x = cof[i][i];

        for (int k = i + 1; k < n; k++)
        {
            double y = cof[k][i];
            for (int j = i; j < n + 1; j++)
            {
                cof[k][j] = cof[k][j] - cof[i][j] * y / x;
            }
        }
    }

    //------back substitution phase------
    for (int i = n - 1; i >= 0; i--)
    {
        double sum = 0;
        for (int j = i + 1; j < n; j++)
        {
            sum += ans[j] * cof[i][j];
        }
        ans[i] = (cof[i][n] - sum) / cof[i][i];
    }
}

void solve(ifstream &fin, ofstream &fout)
{
    int n;
    fin >> n;

    vector<double> x(n), y(n);
    for (int i = 0; i < n; i++)
    {
        fin >> x[i] >> y[i];
    }

    int m;
    fin >> m;

    fout << "Finding Polynomial Equation of order " << m << '\n';

    m++;
    int tot = 2 * m - 2;

    long double sumY = 0;
    for (int i = 0; i < n; i++)
    {
        sumY += y[i];
    }

    vector<long double> sq, xy;
    sq.push_back(n);
    xy.push_back(sumY);

    int p = 1;
    for (int i = 0; i < tot; i++)
    {
        long double sumX = 0, sumXY = 0;
        for (int i = 0; i < n; i++)
        {
            long double q = pow(x[i], p);
            sumX += q;
            sumXY += (q * y[i]);
        }
        p++;
        sq.push_back(sumX);
        xy.push_back(sumXY);
    }

    //-----augmented matrix------
    vector<vector<double>> cof(m, vector<double>(m + 1, 0));

    for (int i = 0; i < m; i++)
    {
        for (int j = 0, k = i; j < m; j++, k++)
        {
            cof[i][j] = sq[k];
        }
        cof[i][m] = xy[i];
    }

    vector<double> ans(m, 0);
    gauss(ans, cof);

    fout << "Polynomial fitted equation: \n";

    fout << "y = ";
    bool first = true;

    for (int i = 0; i < m; i++)
    {
        if (fabs(ans[i]) < 1e-9)
            continue;
        if (!first)
        {
            if (ans[i] > 0)
                fout << " + ";
            else
                fout << " - ";
        }
        else if (ans[i] < 0)
            fout << "-";

        double coef = fabs(ans[i]);
        if (!(coef == 1 && i > 0))
            fout << coef;

        if (i > 0)
        {
            fout << "x";
            if (i > 1)
                fout << "^" << i;
        }

        first = false;
    }

    fout << "\n\n";
}

int main()
{
    ifstream fin("input.txt");
    ofstream fout("output.txt");

    fout << "Least Square Regression Method for Polynomial Equations\nMultiple Testcases\n\n";

    if (!fin)
    {
        fout << "File not found.\n";
        return 0;
    }

    //-----Multiple Testcases----
    int t;
    fin >> t;

    int cse = 1;
    while (t--)
    {
        for (int i = 0; i < 45; i++)
            fout << "=";
        fout << '\n';
        fout << "Test case: " << cse << "\n";
        solve(fin, fout);
        cse++;
    }
}
```
<br>

### ğŸ“ Least Square Regression (Polynomial Equation) Input
```
2
5
1 6
2 11
3 18
4 27
5 38
2
5
0 5
1 5
2 7
3 17
4 41
3
```
<br>

### ğŸ“¤ Least Square Regression (Polynomial Equation) Output
```
Least Square Regression Method for Polynomial Equations
Multiple Testcases

=============================================
Test case: 1
Finding Polynomial Equation of order 2
Polynomial fitted equation: 
y = 3 + 2x + 1x^2

=============================================
Test case: 2
Finding Polynomial Equation of order 3
Polynomial fitted equation: 
y = 5 + 1x - 2x^2 + 1x^3
```
<br>

### ğŸ¯ Accuracy Consideration

- Numerical instability may occur with very high degree polynomials

- The condition number of the normal equations matrix increases with polynomial degree
  
<br>

### â• Advantages

- Can model non-linear relationships effectively

- Provides better fit than linear regression for curved data

- Can approximate many different curve shapes
  
<br>

### â– Disadvantages

- Risk of overfitting with high degree polynomials

- Computationally more expensive than linear regression

- Difficult to determine the optimal polynomial degree

- Numerical instability with high degrees
  
<br>

### ğŸš€ Applications

- Modeling growth curves, fitting experimental data in physics and engineering
 
- Modeling non-linear relationships in data science
<br>

[Back to Top](#-Table-of-Contents)

---


# Interpolation

<br>


## Newton Forward Interpolation Method

<br>

### ğŸ“– Newton Forward Interpolation Method Theory
Newtonâ€™s Forward Interpolation Method is used to estimate the value of a function at a given point using equally spaced data points.  
It is most suitable when the interpolation point lies near the **beginning of the data table**. The method constructs a polynomial using forward differences.

---
<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm
1. Arrange the given data points with equal intervals.
2. Construct the forward difference table.
3. Compute the interpolation parameter:
```
u = (x âˆ’ x0) / h

```
4. Apply Newtonâ€™s forward interpolation formula:
```
y â‰ˆ y0+ uÎ”y0+ u(uâˆ’1)/2! Î”Â²y0+ u(uâˆ’1)(uâˆ’2)/3! Î”Â³y0+...
```
5. Evaluate the polynomial to obtain the interpolated value.
<br>

### ğŸ’» Newton Forward Interpolation Method Code

```cpp
#include<bits/stdc++.h>
#include<fstream>
using namespace std;

double u_cal(int n,double u){
double temp=u;
for(int i=1;i<n;i++){
 temp*=(u-i);
}
return temp;
}

int fact(int n){
    int f=1;
    for(int i=2;i<=n;i++){
        f*=i;
    }
    return f;
}

double nf(vector<vector<double>>& y, vector<double>& x,double val,ofstream& fout){
int n=x.size();

//create difference table

for(int i=1;i<n;i++){
    for(int j=0;j<n-i;j++){
        y[j][i]=y[j+1][i-1]-y[j][i-1];
    }
}

//print forward difference table

fout<<"Forward difference table\n";

for(int i=0;i<n;i++){
    fout<<x[i]<<" ";
    for(int j=0;j<n-i;j++){
        fout<<y[i][j]<<" ";
    }fout<<endl;
}fout<<endl;

double ans=y[0][0];

double u=(val-x[0])/(x[1]-x[0]);

for(int i=1;i<n;i++){
    ans+=(u_cal(i,u)*y[0][i])/fact(i);
}
return ans;
}


int main(){
    ifstream fin("input.txt");
    ofstream fout("output.txt");
int n;
fin>>n;

vector<double> x(n),y0(n);
vector<vector<double>> y(n,vector<double>(n));


for(int i=0;i<n;i++){
    fin>>x[i];
}

for(int i=0;i<n;i++){
    fin>>y0[i];
}

for(int i=0;i<n;i++){
   y[i][0]=y0[i];
}

double val;
fin>>val;

double ans=nf(y,x,val);


fout<<"Print the answer : "<<ans<<endl;
    return 0;
}

```
<br>

### ğŸ“ Newton Forward Interpolation Method Input
```
4
3 5 7 9
180 150 120 90
4
```
<br>

### ğŸ“¤ Newton Forward Interpolation Method Output
```
Print the answer: 165
```
<br>

### ğŸ¯ Accuracy Consideration
- Accuracy depends on the step size **h**.
- Smaller values of **h** give better accuracy.
- Higher-order differences may introduce numerical errors.
- Best results are obtained when the interpolation point is close to the first data value.

---

<br>

### â• Advantages
- Simple and easy to implement.
- Suitable for equally spaced data.
- Efficient when interpolation is required near the beginning of the table.

---
<br>

### â– Disadvantages
- Not suitable for unequally spaced data.
- Accuracy decreases for points far from the starting value.
- Higher-order terms increase computational complexity.

---
<br>

### ğŸš€ Applications
- Numerical analysis problems.
- Engineering and scientific computations.
- Estimation of missing values in tabulated data.
<br>

[Back to Top](#-Table-of-Contents)

---

## Newton Backward Interpolation Method

<br>

### ğŸ“– Newton Backward Interpolation Method Theory
Newtonâ€™s Backward Interpolation Method is used to estimate the value of a function from equally spaced data points when the interpolation point lies near the **end of the data table**.  
It constructs an interpolation polynomial using backward differences.

---
<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm
1. Arrange the given data points with equal intervals.
2. Construct the backward difference table.
3. Compute the interpolation parameter:
```
v = (x âˆ’ xn) / h
```
4. Apply Newtonâ€™s backward interpolation formula:
```
y â‰ˆ yn+ vâˆ‡yn+ v(v+1)/2! âˆ‡Â²yn+ v(v+1)(v+2)/3! âˆ‡Â³yn+...

```
<br>

### ğŸ’» Newton Backward Interpolation Method Code

```cpp
#include <bits/stdc++.h>
#include<fstream>

using namespace std;

float u_calc(float u, int n)
{
    float temp = u;
    for (int i = 1; i < n; i++)
    {
        temp *= (u + i);
    }
    return temp;
}

int fact(int n)
{
    int f = 1;
    for (int i = 2; i <= n; i++)
    {
        f *= i;
    }
    return f;
}

float newton_backward(vector<float> &x, vector<vector<float>> &y, float val,ofstream& fout)
{
    int n = x.size();

    // find difference table

    for (int i = 1; i < n; i++)
    {
        for (int j = n - 1; j >= i; j--)
        {
            y[j][i] = y[j][i - 1] - y[j - 1][i - 1];
        }
    }

    // display backward difference table

    for (int i = 0; i < n; i++)
    {
	   fout<<x[i]<<" ";
        for (int j = 0; j <= i; j++)
        {
            fout << y[i][j] << " ";
        }
        fout << endl;
    }

    float ans = y[n - 1][0];
    float u = (val - x[n - 1]) / (x[1] - x[0]);

    for (int i = 1; i < n; i++)
    {
        ans += (u_calc(u, i) * y[n - 1][i]) / fact(i);
    }

    return ans;
}

int main()
{
    ifstream fin("inp.txt");
    ofstream fout("out.txt");
    int n;
    fin >> n;

    vector<float> x(n), y0(n);

    for (int i = 0; i < n; i++)
    {
        fin >> x[i];
    }

    for (int i = 0; i < n; i++)
    {
        fin >> y0[i];
    }

    vector<vector<float>> y(n, vector<float>(n, 0));

    for (int i = 0; i < n; i++)
    {
        y[i][0] = y0[i];
    }

    float val;
    fin >> val;

    float ans = newton_backward(x, y, val,fout);
    fout << "Print the answer: "<< ans << endl;
    return 0;
} 
```
<br>

### ğŸ“ Newton Backward Interpolation Method Input
```
4
3 5 7 9
180 150 120 90
8
```
<br>

### ğŸ“¤ Newton Backward Interpolation Method Output
```
Print the answer: 105
```
<br>

### ğŸ¯ Accuracy Consideration
- Accuracy depends on the step size **h**.
- Smaller **h** improves precision.
- Higher-order backward differences may increase numerical errors.
- Most accurate when the interpolation point is close to the last data value.

---
<br>

### â• Advantages
- Simple and systematic method.
- Efficient for interpolation near the end of the table.
- Works well with equally spaced data.

---
<br>

### â– Disadvantages
- Not applicable to unequally spaced data.
- Accuracy decreases for points far from the table end.
- Computational effort increases with higher-order terms.

---
<br>

### ğŸš€ Applications
- Numerical computation and data estimation.
- Engineering and scientific problem solving.
- Interpolation of values near the upper boundary of datasets.
<br>

[Back to Top](#-Table-of-Contents)

---

## Newton Divided Difference Interpolation Method

<br>

### ğŸ“– Newton Divided Difference Interpolation Method Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Newton Divided Difference Interpolation Method Code

```cpp
code
```
<br>

### ğŸ“ Newton Divided Difference Interpolation Method Input
```
Input
```
<br>

### ğŸ“¤ Newton Divided Difference Interpolation Method Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

[Back to Top](#-Table-of-Contents)

---



# Solution of Ordinary Differential Equation

<br>



## Runge Kutta Method

<br>

### ğŸ“– Runge Kutta Method Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Runge Kutta Method Code

```cpp
code
```
<br>

### ğŸ“ Runge Kutta Method Input
```
Input
```
<br>

### ğŸ“¤ Runge Kutta Method Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications
- Aplications
<br>

[Back to Top](#-Table-of-Contents)

---



# Integration

<br>



## Simpson 1/3 Rule

<br>

### ğŸ“– Simpson 1/3 Rule Theory

Simpsonâ€™s 1/3 Rule is a numerical method used to approximate the definite integral of a function. It divides the integration interval  `[a, b]` into an even number of subintervals and fits a second-degree (parabolic) polynomial through three consecutive points.The area under the curve is estimated using these parabolas rather than straight lines.This method is more accurate than the trapezoidal rule for smooth functions because it approximates the curve more closely.<br>

**Formula:**
```Integral â‰ˆ (h/3) * [f(xâ‚€) + f(xâ‚™) + 4f(xâ‚) + 2f(xâ‚‚) + 4f(xâ‚ƒ) + â€¦ + 4f(xâ‚™â‚‹â‚) ]```
<br>
Where `h = (b - a)/n`<br>
n must be even.<br>

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

**Step 1**: Define the function `f(x)` to be integrated and input the lower limit (low), upper limit (up), and the step size (h).<br>

**Step 2**: Determine the number of intervals $n$ using the formula:  `n = (up - low)/h`.<br>

**Step 3**: Check if n is an even number. If n is odd, the Simpson's 1/3 Rule cannot be applied (return an error or adjust h).<br>

**Step 4**: Calculate the initial sum by adding the function values at the boundaries:  sum = f(low) + f(up).<br>

**Step 5**: Loop through each interval from i = 1 to n-1 : <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update the current x value: `x = low + (i * h)`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If i is odd: Multiply f(x) by 4 and add to the sum.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If i is even: Multiply f(x) by 2 and add to the sum.<br>

**Step 6**: Multiply the total sum by h/3 to get the final result:<br>  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Result = sum * h/3` .<br>
<br>

### ğŸ’» Simpson 1/3 Rule Code

```cpp
#include<bits/stdc++.h>
using namespace std;

void prnt() {
    cout << "============================================\n";
    cout << "           Simpson 1/3 Method\n";
    cout << "============================================\n\n";
}

void R(){
    cout << "============================================\n";
    cout<<"                    Result\n";
    cout << "============================================\n";
}

void p(){
    cout << "============================================\n";
    cout<<"            The task is completed\n";
    cout << "============================================\n";
}
double f(double x) 
{return (1.0/(1+x*x));}

int main(){

    double low,up,h;
    char c;
    freopen("input.txt", "r", stdin);
    freopen("output.txt", "w", stdout);
    prnt();
    R();
    while(true){
    cin>>low>>up>>h;

   int n = (up-low)/h;

   if(n%2==1){
    cout<<"\t\t\tn must be even\n";
    cin>>c;
    if(c=='q') break;
    else continue;
   }

    double ans = f(low) + f(up);

    int i = 1;
    double x = low;
    for (int i = 1; i < n; i++) {
        x += h;
        if (i%2==1)
            {ans += 4 * f(x);
            }
        else
            {ans += 2 * f(x);
            }
    }
  
    cout<<"\t\t\tThe ans is : "<<ans *(h/3)<<"\n";

    cin>>c;
    if(c=='q') break;
    else continue;
}

   p();

    return 0;
}
```
<br>

### ğŸ“ Simpson 1/3 Rule Input
```
0 1 0.25
Y
0 2 0.5
Y
1 5 0.1
q
```
<br>

### ğŸ“¤ Simpson 1/3 Rule Output
```
============================================
           Simpson 1/3 Method
============================================

============================================
                    Result
============================================
			The ans is : 0.785392
			The ans is : 1.10513
			The ans is : 0.588003
============================================
            The task is completed
============================================
```
<br>

### ğŸ¯ Accuracy Consideration

1. Simpsonâ€™s 1/3 Rule is more accurate than the trapezoidal rule because it approximates the function with parabolic curves instead of straight lines.<br>

2. Using a smaller step size `h` (more subintervals) increases accuracy.<br>

3. Accuracy improves when the function is smooth and continuous over the interval.<br>

4. The error depends on the fourth derivative of the function; smaller higher-order derivatives lead to higher accuracy.<br>

<br>

### â• Advantages

1. Simpson's 1/3 rule provides good accuracy for smooth functions with fewer subintervals compared to the trapezoidal rule.<br>

2. This method is simple and easy to implement.<br>

3. Suitable for a wide range of functions that are continuous and differentiable.<br>

<br>

### â– Disadvantages

1. This method requires an even number of subintervals.<br>

2. Less accurate for functions with discontinuities or rapid changes.<br>

3. Not suitable for functions that are highly oscillatory.<br>
<br>

### ğŸš€ Applications

1. Calculating areas under curves.<br>

2. Determining volumes, work done, and other physical quantities where analytical integration is difficult.<br>

3. Used in computer simulations and numerical analysis to approximate definite integrals.<br>
<br>

[Back to Top](#-Table-of-Contents)

---

## Simpson 3/8 Rule

<br>

### ğŸ“– Simpson 3/8 Rule Theory

Simpsonâ€™s 3/8 Rule is a numerical method used to approximate the definite integral of a function.The method divides the integration interval `[a, b]` into subintervals where the total number `n` is a multiple of 3.It fits a third-degree (cubic) polynomial through four consecutive points to estimate the area under the curve.This method is more accurate than the trapezoidal rule and is specifically designed for cases where the number of intervals is a multiple of 3.<br>

**Formula:**
```Integral â‰ˆ (3h/8) * [f(xâ‚€) + f(xâ‚™) + 3f(xâ‚) + 3f(xâ‚‚) + 2f(xâ‚ƒ) + 3f(xâ‚„) + 3f(xâ‚…) + 2f(xâ‚†) + ...]```<br>

Where `h = (b - a)/n`<br>
`n` must be a multiple of 3<br>

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

**Step 1**: Define the function f(x) to be integrated and input the lower limit (low), upper limit (up), and the step size (h).<br>

**Step 2**: Determine the number of intervals $n$ using the formula:  `n = (up - low)/h`.<br>

**Step 3**: Check if n is a multiple of 3. If `n (mod 3) â‰  0`, the Simpson's 3/8 Rule cannot be applied (return an error or adjust h).<br>

**Step 4**: Calculate the initial sum by adding the function values at the boundaries:  sum = f(low) + f(up).<br>

**Step 5**: Loop through each interval from i = 1 to n-1 : <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update the current x value: `x = low + (i * h)`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If i is multiple of 3: Multiply f(x) by 2 and add to the sum.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Otherwise: Multiply f(x) by 3 and add to the sum.<br>

**Step 6**: Multiply the total sum by 3h/8 to get the final result:<br>  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Result = sum * 3*h/8` .<br>


<br>

### ğŸ’» Simpson 3/8 Rule Code

```cpp
#include <bits/stdc++.h>
using namespace std;

void prnt() {
    cout << "============================================\n";
    cout << "           Simpson 3/8 Method\n";
    cout << "============================================\n\n";
}

void R(){
    cout << "============================================\n";
    cout<<"                    Result\n";
    cout << "============================================\n";
}

void p(){
    cout << "============================================\n";
    cout<<"            The task is completed\n";
    cout << "============================================\n";
}

double f(double x){
    return sqrt(x);
}

int main() {
    double low, up, h;
            char c;

    freopen("input.txt", "r", stdin);
    freopen("output.txt", "w", stdout);
	prnt();

    R();

    while (true) {
        cin >> low >> up >> h;

        int n = (up - low) / h;

        if (n % 3 != 0) {
            cout << "\t\t\tn must be multiple of 3\n";
               cin >> c;
        if (c == 'q') break;
        else continue;
        }

        double ans = f(low) + f(up);
        double x = low;

        for (int i = 1; i < n; i++) {
            x += h;
            if (i % 3 == 0)
                ans += 2 * f(x);
            else
                ans += 3 * f(x);
        }

        ans = ans * (3 * h / 8);
        cout <<"\t\t\tThe ans is : "<< ans << "\n";


        cin >> c;
        if (c == 'q') break;
    }
    p();

    return 0;
}

```
<br>

### ğŸ“ Simpson 3/8 Rule Input
```
0 0.45 0.05
Y
0 4.5 0.5
Y
1 5 0.1
q
```
<br>

### ğŸ“¤ Simpson 3/8 Rule Output
```
============================================
           Simpson 3/8 Method
============================================

============================================
                    Result
============================================
			The ans is : 0.200141
			The ans is : 6.32902
			n must be multiple of 3
============================================
            The task is completed
============================================

```
<br>

### ğŸ¯ Accuracy Consideration

1. The Simpsonâ€™s 3/8 Rule is more accurate than the trapezoidal rule because it approximates the curve using cubic polynomials.<br>

2. Accuracy improves when the function is smooth and continuous over the interval.<br>

3. Using a smaller step size `h` (more subintervals) increases the accuracy.<br>

4. The number of subintervals `n` must be a multiple of `3` to apply the rule properly.<br>

5. If the function has rapid changes or discontinuities, accuracy may decrease.<br>

<br>

### â• Advantages

1. Simpson's `3/8` rule is more accurate than the trapezoidal rule.<br>

2. This method is suitable for smooth and continuous functions.<br>

3. This method can handle integrals over intervals where the number of subintervals is a multiple of `3`.<br>

<br>

### â– Disadvantages

1. Simpson's `3/8` rule requires the number of subintervals `n` to be a multiple of `3`.<br>

2. Less accurate for functions with discontinuities or sharp variations.<br>

3. Slightly more complex calculations compared to Simpsonâ€™s `1/3` rule.<br>

<br>

### ğŸš€ Applications

1. Using simpson's 3/8 rule we can Calculate areas, volumes, and work done in applied problems.<br>

2. This method can Solve problems in applied mathematics where analytical integration is difficult.<br>

3. Useful in computer simulations and numerical analysis for approximating integrals.<br>
<br>

[Back to Top](#-Table-of-Contents)

---




# Differentiation
<br>


## Differentiation Using Forward Interpolation


<br>

### ğŸ“– Differentiation Using Forward Interpolation Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Differentiation Using Forward Interpolation Code

```cpp
code
```
<br>

### ğŸ“ Differentiation Using Forward Interpolation Input
```
Input
```
<br>

### ğŸ“¤ Differentiation Using Forward Interpolation Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications
- Applications
<br>

[Back to Top](#-Table-of-Contents)

---

## Differentiation Using Backward Interpolation


<br>

### ğŸ“– Differentiation Using Backward Interpolation Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Differentiation Using Backward Interpolation Code

```cpp
code
```
<br>

### ğŸ“ Differentiation Using Backward Interpolation Input
```
Input
```
<br>

### ğŸ“¤ Differentiation Using Backward Interpolation Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications
- Applications
<br>

[Back to Top](#-Table-of-Contents)

---


































































































