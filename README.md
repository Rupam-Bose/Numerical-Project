# ğŸ“˜ Table of Contents

<details>
<summary><b><a href = "#Solution-of-Linear-Equations">Solution of Linear Equations</a></b></summary>
<br>

<details>
<summary><a href = "#Gauss-Elimination-Method">ğŸ”¹ Gauss Elimination Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Gauss-Elimination-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Gauss-Elimination-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Gauss-Elimination-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Gauss-Elimination-Method-Output)  

</details>
<br>

<details>
<summary><a href = "#Gauss-Jordan-Elimination-Method">ğŸ”¹ Gauss Jordan Elimination Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Gauss-Jordan-Elimination-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Gauss-Jordan-Elimination-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Gauss-Jordan-Elimination-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Gauss-Jordan-Elimination-Method-Output)  

</details>
<br>

<details>
<summary><a href = "#LU-Factorization-Method">ğŸ”¹ LU Factorization Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-LU-Factorization-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-LU-Factorization-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-LU-Factorization-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-LU-Factorization-Method-Output)  

</details>
<br>

<details>
<summary><a href = "#Matrix-Inversion-Method">ğŸ”¹ Matrix Inversion Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Matrix-Inversion-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Matrix-Inversion-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Matrix-Inversion-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ğŸ“¤ [Output](#-Matrix-Inversion-Method-Output)  

</details>

</details>


<details>
<summary><b><a href = "#Solution-of-Non-Linear-Equations">Solution of Non-Linear Equations</a></b></summary>
<br>

<details>
<summary><a href = "#Bisection-Method">ğŸ”¹ Bisection Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Bisection-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Bisection-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Bisection-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Bisection-Method-Output)  

</details>
<br>

<details>
<summary><a href = "#False-Position-Method">ğŸ”¹ False Position Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-False-Position-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-False-Position-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-False-Position-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-False-Position-Method-Output)  

</details>
<br>


<details>
<summary><a href = "#Newton-Raphson-Method">ğŸ”¹ Newton-Raphson Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Newton-Raphson-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Newton-Raphson-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Newton-Raphson-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Newton-Raphson-Method-Output)  

</details>
<br>

<details>
<summary><a href = "#Secant-Method">ğŸ”¹ Secant Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Secant-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Secant-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Secant-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Secant-Method-Output)  

</details>
<br>

</details>

<details>
<summary><b><a href = "#Curve-Fitting">Curve Fitting: Regression</a></b></summary>
<br>

<details>
<summary><a href = "#Least-Square-Regression-Linear-Equation">ğŸ”¹ Least Square Regression (Linear Equation)</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Least-Square-Regression-Linear-Equation-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Least-Square-Regression-Linear-Equation-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Least-Square-Regression-Linear-Equation-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Least-Square-Regression-Linear-Equation-Output)  

</details>
<br>

<details>
<summary><a href = "#Least-Square-Regression-Transcendental-Equation">ğŸ”¹ Least Square Regression (Transcendental Equation)</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Least-Square-Regression-Transcendental-Equation-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Least-Square-Regression-Transcendental-Equation-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Least-Square-Regression-Transcendental-Equation-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Least-Square-Regression-Transcendental-Equation-Output)  

</details>
<br>


<details>
<summary><a href = "#Least-Square-Regression-Polynomial-Equation">ğŸ”¹ Least Square Regression (Polynomial Equation)</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Least-Square-Regression-Polynomial-Equation-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Least-Square-Regression-Polynomial-Equation-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Least-Square-Regression-Polynomial-Equation-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Least-Square-Regression-Polynomial-Equation-Output)  

</details>
<br>

</details>

<details>
<summary><b><a href = "#Interpolation">Interpolation</a></b></summary>
<br>

<details>
<summary><a href = "#Newton-Forward-Interpolation-Method">ğŸ”¹ Newton Forward Interpolation Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Newton-Forward-Interpolation-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Newton-Forward-Interpolation-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Newton-Forward-Interpolation-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Newton-Forward-Interpolation-Method-Output)  

</details>
<br>

<details>
<summary><a href = "#Newton-Backward-Interpolation-Method">ğŸ”¹ Newton Backward Interpolation Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Newton-Backward-Interpolation-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Newton-Backward-Interpolation-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Newton-Backward-Interpolation-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Newton-Backward-Interpolation-Method-Output)  

</details>
<br>


<details>
<summary><a href = "#Newton-Divided-Difference-Interpolation-Method">ğŸ”¹ Newton Divided Difference Interpolation Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Newton-Divided-Difference-Interpolation-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Newton-Divided-Difference-Interpolation-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Newton-Divided-Difference-Interpolation-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Newton-Divided-Difference-Interpolation-Method-Output)  

</details>
<br>

</details>

<details>
<summary><b><a href = "#Solution-of-Ordinary-Differential-Equation">Solution of Ordinary Differential Equation</a></b></summary>
<br>

<details>
<summary><a href = "#Runge-Kutta-Method">ğŸ”¹ Runge Kutta Method</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Runge-Kutta-Method-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Runge-Kutta-Method-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Runge-Kutta-Method-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Runge-Kutta-Method-Output)  

</details>
<br>

</details>

<details>
<summary><b><a href = "#Integration">Integration</a></b></summary>
<br>

<details>
<summary><a href = "#Simpson-13-Rule">ğŸ”¹ Simpson 1/3 Rule</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Simpson-13-Rule-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Simpson-13-Rule-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Simpson-13-Rule-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Simpson-13-Rule-Output)  

</details>
<br>
<details>
<summary><a href = "#Simpson-38-Rule">ğŸ”¹ Simpson 3/8 Rule</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Simpson-38-Rule-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Simpson-38-Rule-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Simpson-38-Rule-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Simpson-38-Rule-Output)  

</details>
<br>

</details>

<details>
<summary><b><a href = "#Differentiation">Differentiation</a></b></summary>
<br>

<details>
<summary><a href = "#Numerical-Differentiation">ğŸ”¹ Numerical Differentiation</a></summary>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“– [Theory](#-Numerical-Differentiation-Theory)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ’» [Code](#-Numerical-Differentiation-Code)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“ [Input](#-Numerical-Differentiation-Input)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ğŸ“¤ [Output](#-Numerical-Differentiation-Output)  

</details>
<br>
</details>



---

<br>

# Solution of Linear Equations

<br>
---

## Gauss Elimination Method

<br>

### ğŸ“– Gauss Elimination Method Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Gauss Elimination Method Code

```cpp
code
```
<br>

### ğŸ“ Gauss Elimination Method Input
```
Input
```
<br>

### ğŸ“¤ Gauss Elimination Method Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---

## Gauss-Jordan Elimination Method

<br>

### ğŸ“– Gauss-Jordan Elimination Method Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Gauss-Jordan Elimination Method Code

```cpp
code
```
<br>

### ğŸ“ Gauss-Jordan Elimination Method Input
```
Input
```
<br>

### ğŸ“¤ Gauss-Jordan Elimination Method Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---

## LU-Factorization Method

<br>

### ğŸ“– LU-Factorization Method Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» LU-Factorization Method Code

```cpp
code
```
<br>

### ğŸ“ LU-Factorization Method Input
```
Input
```
<br>

### ğŸ“¤ LU-Factorization Method Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---

## Matrix Inversion Method

<br>

### ğŸ“– Matrix Inversion Method Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Matrix Inversion Method Code

```cpp
code
```
<br>

### ğŸ“ Matrix Inversion Method Input
```
Input
```
<br>

### ğŸ“¤ Matrix Inversion Method Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---

# Solution of Non-Linear Equations

A non-linear equation is an equation in which the highest power of the variable is greater than 1 or the variables appear in non-linear forms (squared, cubed, trigonometric, logarithmic, etc). <br>
Example: x<sup>2</sup> + 4x - 10 = 0<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;e<sup>x</sup> - 4x = 0<br>
There are some techniques to solve the non-linear equations. Such as  

-> **Bracketing Methods** <br>

-> **Open Methods** <br>


**Bracketing methods** are a class of numerical algorithms used to find the roots of a non-linear equation f(x) = 0. They are called **bracketing** methods because they require two initial guesses, a and b, that "bracket" or surround the root between them. <br>

Popular Bracketing Method: <br>

1. **Bisection Method** <br>

2. **False Position Method**<br>

**Open methods** are iterative techniques used to find the roots of a non-linear equation f(x) = 0 without needing an interval that brackets the root. Open methods typically use a single starting value or two starting values.<br>

Popular Open Method: <br>

1. **Newton-Raphson Method**  <br>

2. **Secant Method**<br>

<br>

## Bisection Method

<br>

### ğŸ“– Bisection Method Theory

The Bisection Method is one of the simplest numerical techniques for finding the solution of a non-linear equation f(x) = 0. It is also known as binary chopping and half interval method. <br>

The main concept of this method is if the function f(x) is real and continuous in the interval a<x<b and f(a) and f(b) are of opposite sign that is , f(a) *f(b) < 0 then there exist at least one solution in this interval. <br>

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

**Step 1**: Choose two real numbers $x_1$ and $x_2$ such that the function changes sign over the interval: f($x_1$)*f($x_2$) < 0<br>

**Step 2**: Calculate the midpoint $x_0$ of the current interval: $x_0$ = ($x_1$ + $x_2$)/2<br>

**Step 3**: Evaluate the function at the midpoint: f($x_0$).<br>

**Step 4**:  If f($x_0$) = 0, then $x_0$ is the exact root. And Stop the iteration.<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If f($x_0$) * f($x_1$) < 0, the root lies between $x_1$ and $x_0$. Set $x_2$ = $x_0$.<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If f($x_0$) * f($x_2$) < 0, the root lies between $x_0$ and $x_2$. Set $x_1$ = $x_0$.<br>

**Step 5**: Repeat Steps 2 through 4 until the relative error satisfies the stopping criterion:
|($x_2$ - $x_1$)/$x_2$| < e <br>



<br>

### ğŸ’» Bisection Method Code

```cpp
#include<bits/stdc++.h>
using namespace std;

void prnt() {
    cout << "============================================\n";
    cout << "              Bisection Method\n";
    cout << "============================================\n\n";
}

double f(double x, vector<double>& coef){
    double res = 0.0;
    int n = coef.size();
    for(int i=0;i<n;i++){
        res += coef[i]*pow(x,n-1-i);
    }
    return res;

}

void printPolynomial(vector<double> coef){
      cout << "\nPolynomial function:\n";

      int n = coef.size() - 1;

    for (int i = 0; i <= n; i++) {
        int power = n - i;

        if (coef[i] == 0) continue;

        if (i != 0 && coef[i] > 0)
            cout << " + ";
        else if(i !=0 && coef[i]<0)
            cout<<" - ";    

        if (power == 0)
            cout << fabs(coef[i]);
        else if (power == 1)
            cout << fabs(coef[i]) << "x";
        else if(fabs(coef[i]) >1)
            cout << fabs(coef[i]) << "x^" << power;
        else 
            cout << "x^" << power;

    }

    cout << " = 0\n";
    return;
}

double bisectionMethod(vector<double>& coef, double L, double R, double e){
    double pos = R, neg = L, mid, x;

    do{
         mid = (pos+neg)/2.0;
         x = f(mid,coef);
         if(f(neg,coef)*x<0.0){pos = mid;}
         else
            neg = mid;
    }while(fabs(x)>e);

    return mid;
}

int main(){

    double x,pos,neg,e = 0.001,mid;
    freopen("input.txt", "r", stdin);
    freopen("output.txt", "w", stdout);
    prnt();
    int n;

    cin>>n;

    vector<double>coef(n+1);


    for (int i = 0; i <= n; i++) {
        cin >> coef[i];
    }
    double L, R, h;

    cin >> L >> R;


    cin >> h;

    double x1 = L, x2 = L + h;
    bool found = false;

    vector<double> roots;
    while (x2 <= R) {
        if (f(x1, coef) * f(x2, coef) < 0) {
            pos = x2;
            neg = x1;
            roots.push_back(bisectionMethod(coef, x1, x2, e));
        x1 = x2;
        x2 += h;
        continue;

        }
        x1 = x2;
        x2 += h;
    }

    printPolynomial(coef);


    cout << "\nRoots found in the given range:\n";int i = 0;

    for (double root : roots) {
        i++;
        cout <<"\tx"<<i<<" = "<< fixed << setprecision(6) << root << endl;
    }

    return 0;
}

```
<br>

### ğŸ“ Bisection Method Input
```
6
1 -21 175 -735 1624 -1764 720
-5 10
0.1

```
<br>

### ğŸ“¤ Bisection Method Output
```
============================================
              Bisection Method
============================================


Polynomial function:
x^6 - 21x^5 + 175x^4 - 735x^3 + 1624x^2 - 1764x + 720 = 0

Roots found in the given range:
	x1 = 1.000006
	x2 = 2.000024
	x3 = 2.999951
	x4 = 4.000049
	x5 = 4.999976
	x6 = 6.000006

```
<br>

### ğŸ¯ Accuracy Consideration

1. Choose a small tolerance $E$ as the stopping criterion.<br>

2. Ensure f($x_1$) * f($x_2$) < 0 before starting to guarantee a root exists within the interval.<br>

3. Avoid using very large initial intervals to minimize unnecessary computations and potential overflow.<br>

4. The function must be continuous within the closed interval [$x_1$, $x_2$] for the method to be valid.<br>

5. Stop the algorithm only when the interval width or the relative error is sufficiently small.<br>

<br>

### â• Advantages

1. This method is Simple and Easy to Understand.<br>

2. If the function changes sign over [a, b], the method always converges to a root.<br>

3. Works with only functional value not the differential value. <br>

4. Bisection method Works on Any Continuous Function<br> 

5. After n iterations, the maximum error is   (b-a)/2<sup>n</sup><br>
<br>

### â– Disadvantages

1. Bisection method takes many iterations. Thatâ€™s why this method is slower than Newton-Raphson Method. <br>

2. Bisection method only works when f(a) * f(b)<0.It Fails when both endpoints have same sign. <br>

3. If function touches the axis without crossing the axis, bisection may unable to solve it. <br>

4. This method Can detect only one root in an interval. <br>

<br>

### ğŸš€ Applications

1. Finding roots of transcendental and algebraic equations where analytical solutions are unavailable.<br>

2. Calculating critical values in thermodynamics, fluid dynamics.<br>

3. Solving for balance or equilibrium states in systems influenced by non-linear physical forces.<br>

4. Finding the highest or lowest points of a curve by locating the roots of the function's derivative.<br>
<br>

---

## False Position Method

<br>

### ğŸ“– False Position Method Theory

The False Position Method, also called Regula Falsi, is a numerical technique used to find the real root of a nonlinear equation 
f(x) = 0.It is a bracketing method, meaning it starts with two initial points $x_1$ and $x_2$ such that: f($x_1$) * f($x_2$) < 0.This indicates that the function changes sign and a root lies between them. The formula for this method is : <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $x_0$ = $x_2$ - (f($x_2$)($x_1$ - $x_2$))(f($x_1$) - f($x_2$)).



<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

**Step 1**: Choose two real numbers $x_1$ and $x_2$ such that the function changes sign over the interval: f($x_1$) * f(x_2) < 0 and Define a stopping criterion $E$.<br>

**Step 2**: Calculate the estimate for the root $x_0$ using the interpolation formula: <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$x_0$ = $x_1$ - f($x_1$) * ($x_2$ - $x_1$)(f($x_2$) - f($x_1$).<br>

**Step 3**: Find the value of the function at the new estimate: f($x_0$).<br>

**Step 4**: If f($x_0$) = 0, then the root is $x_0$ and Stop the iteration.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If f($x_0$) * f($x_1$) < 0, then set $x_2$ = $x_0$.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If f($x_0$) * f($x_2$) < 0, then set $x_1$ = $x_0$.<br>

**Step 5**: Return to Step 2 and repeat until the relative error satisfies the stopping criterion:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;abs($x_2$ - $x_1$)/$x_2$) < E.
<br>

### ğŸ’» False Position Method Code

```cpp
#include<bits/stdc++.h>
using namespace std;

void prnt() {
    cout << "============================================\n";
    cout << "           False Position Method\n";
    cout << "============================================\n\n";
}

double f(double x, vector<double>& coef){
    double res = 0.0;
    int n = coef.size();
    for(int i=0;i<n;i++){
        res += coef[i]*pow(x,n-1-i);
    }
    return res;

}

void printPolynomial(vector<double> &coef){
      cout << "\nPolynomial function:\n";

      int n = coef.size() - 1;

    for (int i = 0; i <= n; i++) {
        int power = n - i;

        if (coef[i] == 0) continue;

        if (i != 0 && coef[i] > 0)
            cout << " + ";
        else if(i !=0 && coef[i]<0)
            cout<<" - ";    

        if (power == 0)
            cout << fabs(coef[i]);
        else if (power == 1)
            cout << fabs(coef[i]) << "x";
        else if(fabs(coef[i]) >1)
            cout << fabs(coef[i]) << "x^" << power;
        else 
            cout << "x^" << power;

    }

    cout << " = 0\n";
    return;
}

float false_position(vector<double>& coef,double x1, double x2, double e){
    double x0,fx0;
    do{
         x0 = x1 - f(x1, coef) * (x2 - x1) /
                    (f(x2, coef) - f(x1, coef));

    fx0 = f(x0,coef);
    if(fx0 == 0.0) return x0;

    else if(f(x1,coef)*fx0<0.0) x2 = x0;
    else x2 = x0;
    }while(abs(fx0)>e);

    return x0;
}

int main(){



    double x,pos,neg,e = 0.001,mid;
    freopen("input.txt", "r", stdin);
    freopen("output.txt", "w", stdout);
    prnt();
    int n;
    cin>>n;

    vector<double>coef(n+1);


    for (int i = 0; i <= n; i++) {
        cin >> coef[i];
    }
    double L, R, h;

    cin >> L >> R;

    cin >> h;

    double x1 = L, x2 = L + h;

    vector<double> roots;
    while (x2 <= R) {
        if (f(x1, coef) * f(x2, coef) < 0) {
            pos = x2;
            neg = x1;
            roots.push_back(false_position(coef, x1, x2, e));
        x1 = x2;
        x2 += h;
        continue;

        }
        x1 = x2;
        x2 += h;
    }

    printPolynomial(coef);


    cout << "\nRoots found in the given range:\n";int i = 0;
    for (double root : roots) {
        i++;
        cout <<"\tx"<<i<<" = "<< fixed << setprecision(6) << root << endl;
    }

    return 0;
}


```
<br>

### ğŸ“ False Position Method Input
```
6
1 -21 175 -735 1624 -1764 720
-5 10
0.1

```
<br>

### ğŸ“¤ False Position Method Output
```
============================================
           False Position Method
============================================


Polynomial function:
x^6 - 21x^5 + 175x^4 - 735x^3 + 1624x^2 - 1764x + 720 = 0

Roots found in the given range:
	x1 = 1.000000
	x2 = 2.000000
	x3 = 3.000000
	x4 = 4.000000
	x5 = 5.000000
	x6 = 6.000000

```
<br>

### ğŸ¯ Accuracy Consideration

1. Choose a sufficiently small tolerance ğ¸.<br>

2. Ensure the initial interval satisfies f($x_1$) * f($x_2$) < 0.<br>

3. Need to use double-precision floating values.<br>

4. Check sign change carefully at each step.<br>

5. Ensure the function is continuous in the interval.<br>

<br>

### â• Advantages

1. Guaranteed convergence if f($x_1$) * f($x_2$) < 0.<br>

2. The algorithm always keeps the root trapped within the interval, providing high reliability.<br>

3. Faster than the Bisection method in many cases.<br>

4. The medthod is Simple and easy to implement.<br>

5. No derivative needed.

<br>

### â– Disadvantages

1. Depending on the curve of the function, convergence can become slow.<br>

2. One endpoint may remain fixed for many iterations.<br>

3. If the function values are not well-balanced across the interval, the process can stagnate.<br>

4. For certain non-linear shapes, it may still require a high number of iterations to achieve extreme precision.<br>

<br>

### ğŸš€ Applications

1. Solving nonlinear algebraic equations.<br>

2. Finding roots in engineering design problems.<br>

3. Solving for equilibrium points in systems involving non-linear forces or complex potential energy surfaces.<br>

4. Identifying the local extrema of a function by finding the roots of its first derivative.<br>

<br>

---

## Newton-Raphson Method

<br>

### ğŸ“– Newton-Raphson Method Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Newton-Raphson Method Code

```cpp
code
```
<br>

### ğŸ“ Newton-Raphson Method Input
```
Input
```
<br>

### ğŸ“¤ Newton-Raphson Method Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---

## Secant Method

<br>

### ğŸ“– Secant Method Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Secant Method Code

```cpp
code
```
<br>

### ğŸ“ Secant Method Input
```
Input
```
<br>

### ğŸ“¤ Secant Method Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---



# Curve-Fitting

<br>


## Least Square Regression (Linear Equation)

<br>

### ğŸ“– Least Square Regression (Linear Equation) Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Least Square Regression (Linear Equation) Code

```cpp
code
```
<br>

### ğŸ“ Least Square Regression (Linear Equation) Input
```
Input
```
<br>

### ğŸ“¤ Least Square Regression (Linear Equation) Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---

## Least Square Regression (Transcendental Equation)

<br>

### ğŸ“– Least Square Regression (Transcendental Equation) Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Least Square Regression (Transcendental Equation) Code

```cpp
code
```
<br>

### ğŸ“ Least Square Regression (Transcendental Equation) Input
```
Input
```
<br>

### ğŸ“¤ Least Square Regression (Transcendental Equation) Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---

## Least Square Regression (Polynomial Equation)

<br>

### ğŸ“– Least Square Regression (Polynomial Equation) Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Least Square Regression (Polynomial Equation) Code

```cpp
code
```
<br>

### ğŸ“ Least Square Regression (Polynomial Equation) Input
```
Input
```
<br>

### ğŸ“¤ Least Square Regression (Polynomial Equation) Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---


# Interpolation

<br>


## Newton Forward Interpolation Method

<br>

### ğŸ“– Newton Forward Interpolation Method Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Newton Forward Interpolation Method Code

```cpp
code
```
<br>

### ğŸ“ Newton Forward Interpolation Method Input
```
Input
```
<br>

### ğŸ“¤ Newton Forward Interpolation Method Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---

## Newton Backward Interpolation Method

<br>

### ğŸ“– Newton Backward Interpolation Method Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Newton Backward Interpolation Method Code

```cpp
code
```
<br>

### ğŸ“ Newton Backward Interpolation Method Input
```
Input
```
<br>

### ğŸ“¤ Newton Backward Interpolation Method Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---

## Newton Divided Difference Interpolation Method

<br>

### ğŸ“– Newton Divided Difference Interpolation Method Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Newton Divided Difference Interpolation Method Code

```cpp
code
```
<br>

### ğŸ“ Newton Divided Difference Interpolation Method Input
```
Input
```
<br>

### ğŸ“¤ Newton Divided Difference Interpolation Method Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---



# Solution of Ordinary Differential Equation

<br>



## Runge Kutta Method

<br>

### ğŸ“– Runge Kutta Method Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Runge Kutta Method Code

```cpp
code
```
<br>

### ğŸ“ Runge Kutta Method Input
```
Input
```
<br>

### ğŸ“¤ Runge Kutta Method Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---



# Integration

<br>



## Simpson 1/3 Rule

<br>

### ğŸ“– Simpson 1/3 Rule Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Simpson 1/3 Rule Code

```cpp
code
```
<br>

### ğŸ“ Simpson 1/3 Rule Input
```
Input
```
<br>

### ğŸ“¤ Simpson 1/3 Rule Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---

## Simpson 3/8 Rule

<br>

### ğŸ“– Simpson 3/8 Rule Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Simpson 3/8 Rule Code

```cpp
code
```
<br>

### ğŸ“ Simpson 3/8 Rule Input
```
Input
```
<br>

### ğŸ“¤ Simpson 3/8 Rule Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---



# Differentiation
<br>


## Numerical Differentiation

<br>

### ğŸ“– Numerical Differentiation Theory

<br>

### ğŸ”¢ Mathematical Representation

<br>

### ğŸ¤– Algorithm

<br>

### ğŸ’» Numerical Differentiation Code

```cpp
code
```
<br>

### ğŸ“ Numerical Differentiation Input
```
Input
```
<br>

### ğŸ“¤ Numerical Differentiation Output
```
Output
```
<br>

### ğŸ¯ Accuracy Consideration

<br>

### â• Advantages

<br>

### â– Disadvantages

<br>

### ğŸš€ Applications

<br>

---

































































































